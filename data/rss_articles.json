[
  {
    "title": "Aspora gets $50M from Sequoia to build remittance and banking solutions for Indian diaspora",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nIndia has been one of the top recipients of remittances in the world for more than a decade. Inward remittances jumped from $55.6 billion in 2010-11 to $118.7 billion in 2023-24, according to data fromthe country’s central bank. The bank projects that figure will reach $160 billion in 2029.\nThis means there is an increasing market for digitalized banking experiences for non-resident Indians(NRIs), ranging from remittances to investing in different assets back home.\nAspora(formerly Vance) is trying to build a verticalized financial experience for the Indian diaspora by keeping convenience at the center. While a lot of financial products are in its future roadmap, the company currently focuses largely on remittances.\n“While multiple financial products for non-resident Indians exist, they don’t know about them because there is no digital journey for them. They possibly use the same banking app as residents, which makes it harder for them to discover products catered towards them,” Garg said.\nIn the last year, the company has grown the volume of remittances by 6x — from $400 million to $2 billion in yearly volume processed.\nWith this growth, the company has attracted a lot of investor interest. It raised $35 million in Series A funding last December — which was previously unreported — led by Sequoia with participation from Greylock, Y Combinator, Hummingbird Ventures, and Global Founders Capital. The round pegged the company’s valuation at $150 million. In the four months following, the company tripled its transaction volume, prompting investors to put in more money.\nThe company announced today it has raised $50 million in Series B funding, co-led by Sequoia and Greylock, with Hummingbird, Quantum Light Ventures, and Y Combinator also contributing to the round. The startup said this round values the company at $500 million. The startup has raised over $99 million in funding to date.\nAfter pivoting from being Pipe.com for India, the company started by offering remittance for NRIs in the U.K. in 2023 and has expanded its presence in other markets, including Europe and the United Arab Emirates. It charges a flat fee for money transfer and offers a competitive rate. Now it also allows customers to invest in mutual funds in India. The startup markets its exchange rates as “Google rate” as customers often search for currency conversion rates, even though they may not reflect live rates.\nThe startup is also set to launch in the U.S., one of the biggest remittance corridors to India, next month. Plus, it plans to open up shop in Canada, Singapore, and Australia by the fourth quarter of this year.\nGarg, who grew up in the UAE, said that remittances are just the start, and the company wants to build out more financial tools for NRIs.\n“We want to use remittances as a wedge and build all the financial solutions that the diaspora needs, including banking, investing, insurance, lending in the home country, and products that help them take care of their parents,” he told TechCrunch.\nHe added that a large chunk of money that NRIs send home is for wealth creation rather than family sustenance. The startup said that 80% of its users are sending money to their own accounts back home.\nIn the next few months, the company is launching a few products to offer more services. This month, it plans to launch a bill payment platform to let users pay for services like rent and utilities. Next month, it plans to launch fixed deposit accounts for non-resident Indians that allow them to park money in foreign currency. By the end of the year, it plans to launch a full-stack banking account for NRIs that typically takes days for users to open. While these accounts can help the diaspora maintain their tax status in India, a lot of people use a family member’s account because of the cumbersome process, and Aspora wants to simplify this.\nApart from banking, the company also plans to launch a product that would help NRIs take care of their parents back home by offering regular medical checkups, emergency care coverage, and concierge services for other assistance.\nBesides global competitors like Remitly and Wise, the company also has India-based rivals like Abound,which was spun off from Times Internet.\nSequoia’s Luciana Lixandru is confident that Aspora’s execution speed and verticalized solution will give it an edge.\n“Speed of execution, for me, is one of the main indicators in the early days of the future success of a company,” she told TechCrunch over a call. “Aspora moves fast, but it is also very deliberate in building corridor by corridor, which is very important in financial services.”\n\nTopics\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/15/aspora-gets-50m-to-build-remittance-solutions-for-indian-diaspora/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-16T05:00:00Z"
  },
  {
    "title": "The U.S. Navy is more aggressively telling startups, ‘We want you’",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nWhile Silicon Valley executives like those from Palantir, Meta, and OpenAI are grabbing headlines for trading theirBrunelloCucinellivestsforArmy Reserve uniforms, a quieter transformation has been underway in the U.S. Navy.\nHow so? Well, the Navy’s chief technology officer, Justin Fanelli, says he has spent the last two and a half years cutting through the red tape and shrinking the protracted procurement cycles that once made working with the military a nightmare for startups. The efforts represent a less visible but potentially more meaningful remaking that aims to see the government move faster and be smarter about where it’s committing dollars.\n“We’re more open for business and partnerships than we’ve ever been before,” Fanelli told TechCrunch in a recent Zoom interview. “We’re humble and listening more than before, and we recognize that if an organization shows us how we can do business differently, we want that to be a partnership.”\nRight now, many of these partnerships are being facilitated through what Fanelli calls the Navy’s innovation adoption kit, a series of frameworks and tools that aim to bridge the so-called Valley of Death, where promising tech dies on its path from prototype to production. “Your granddaddy’s government had a spaghetti chart for how to get in,” Fanelli said. “Now it’s a funnel, and we are saying, if you can show that you have outsized outcomes, then we want to designate you as an enterprise service.”\nIn one recent case, the Navy went from a Request for Proposal (RFP) to pilot deployment in under six months with Via, an eight-year-old, Somerville, Mass.-based cybersecurity startup that helps big organizations protect sensitive data and digital identities through, in part, decentralization, meaning the data isn’t stored in one central spot that can be hacked. (Another of Via’s clients is the U.S. Air Force.)\nThe Navy’s new approach operates on what Fanelli calls a “horizon” model, borrowed and adapted from McKinsey’s innovation framework. Companies move through three phases: evaluation, structured piloting, and scaling to enterprise services. The key difference from traditional government contracting, Fanelli says, is that the Navy now leads with problems rather than predetermined solutions.\n“Instead of specifying, ‘Hey, we’d like this problem solved in a way that we’ve always had it,’ we just say, ‘We have a problem, who wants to solve this, and how will you solve it?’” Fanelli said.\nFanelli’s drive to overhaul Navy tech is personal. Originally a scholarship cadet in the Air Force studying electrical engineering, he was disqualified from military service due to a lung issue. Determined to serve anyway, he chose the Navy over private sector offers more than 20 years ago because he “wanted to be around people in uniform.” Since then, his career has spanned roles across defense, intelligence, DARPA, and open source initiatives, before returning to the Department of the Navy.\nThe change he’s overseeing is opening doors to companies that previously never considered government work and may have thought it a waste of time to try. Fanelli points, for example, to one competition run through the Defense Innovation Unit (DIU), wherein the Navy expected a handful of bidders for a niche cybersecurity challenge but received nearly 100 responses – many from companies that had never worked with the DoD before but were already solving similar problems in the private sector.\nFanelli says his team has documented dozens of success stories altogether, including a venture-backed startup that used robotic process automation to zip through a two-year invoice backlog in just a couple of weeks. Another example involved rolling out network improvements to an aircraft carrier that saved 5,000 sailor hours in the first month alone.\n“That not just changed their availability, but it changed their morale, esprit de corps, how much time they could spend doing other tasks,” Fanelli noted, explaining that time saved is one of five metrics that the Navy uses to measure the success of a pilot program. The other four are operational resilience, cost per user, adaptability, and user experience.\nAs for what the Navy is looking for right now, Fanelli outlined several high-priority areas, including AI, where the service is actively talking with teams. The Navy apparently wants to accelerate AI adoption beyond basic generative AI use cases into more agentic applications for everything from onboarding and personnel management to data processing on ships. He also cited “alternative” GPS, explaining that the Navy is quickly adopting alternative precision navigation and timing software, particularly for integration with unmanned systems. And he mentioned “legacy system modernization,” saying that some of the aging technology that the Navy is looking to update includes air traffic control infrastructure and ship-based systems.\nSo how much money is it looking to put to work each year? Fanelli said he wasn’t at liberty to provide specific budget breakdowns, but he said the Navy currently allocates single-digit percentages to emerging and commercial technology versus traditional defense contractors — a balance that he expects to evolve significantly as AI continues to advance.\nAs for the most common reason that promising technologies fail when trialed, he said it isn’t necessarily because of technical shortcomings. Instead, he said, the Navy operates on long budget cycles, and if a new solution doesn’t replace or “turn off” an existing system, funding becomes problematic.\n“If we’re getting benefit and we’re measuring that benefit, but there’s no money [getting to the startup] in a year and a half — that’s a really bad story for their investors and our users,” Fanelli explained. “Sometimes it’s a zero sum game. Sometimes it’s not. And if we’re going to flip the public-private sector to more private and ride that wave, we do have a lot of technical debt that we need to cut anchor on.”\nBefore ending our call, we asked Fanelli if the Trump administration’s “America first” policies are impacting these processes in any way. Fanelli answered that the current focus on domestic manufacturing aligns well with the Navy’s “resilience” goals. (Here, he pointed to ongoing initiatives like digital twins, additive manufacturing, and on-site production capabilities that can reduce supply chain dependencies.)\nEither way, the Navy’s message for entrepreneurs and investors is very clearly that it’s a genuine alternative to traditional commercial markets, and it’s a pitch that appears to be gaining traction in Silicon Valley, where there’s growing receptiveness to partnering with the U.S. government.\nSaid Meta CTO Andrew Bosworthat a recent Bloomberg eventin San Francisco: “There’s a much stronger patriotic underpinning than I think people give Silicon Valley credit for.”\nIt’s a marked change from the more skeptical stance that characterized much of the Valley in previous years, as longtime industry observers can attest. Now, Fanelli — who has been making the rounds, taking with business media outlets and podcast interviewers — hopes to attract more of that interest to the Navy specifically. He told TechCrunch, “I would invite anyone who wants to serve the greater mission from a solution perspective to lean in and to join us in this journey.”\nIf you’re interested in hearing our full conversation with Fanelli, you can check it outright here.\nTopics\nEditor in Chief & General Manager\nLoizos has been reporting on Silicon Valley since the late ’90s, when she joined the original Red Herring magazine. Previously the Silicon Valley Editor of TechCrunch, she was named Editor in Chief and General Manager of TechCrunch in September 2023. She’s also the founder of StrictlyVC, a daily e-newsletter and lecture series acquired by Yahoo in August 2023 and now operated as a sub brand of TechCrunch.\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nAspora gets $50M from Sequoia to build remittance and banking solutions for Indian diaspora\nThe U.S. Navy is more aggressively telling startups, ‘We want you’\nSpiraling with ChatGPT\nTaiwan places export controls on Huawei and SMIC\nAlexa von Tobel has high hopes for ‘fintech 3.0’\nGoogle reportedly plans to cut ties with Scale AI\nHow to delete your 23andMe data\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/15/the-u-s-navy-is-more-aggressively-telling-startups-we-want-you/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-15T23:40:06Z"
  },
  {
    "title": "Spiraling with ChatGPT",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nPosted:\nChatGPT seems to have pushed some users towards delusional or conspiratorial thinking, or at least reinforced that kind of thinking, according toa recent feature in The New York Times.\nFor example, a 42-year-old accountant named Eugene Torres described asking the chatbot about “simulation theory,” with the chatbot seeming to confirm the theory and tell him that he’s “one of theBreakers— souls seeded into false systems to wake them from within.”\nChatGPT reportedly encouraged Torres to give up sleeping pills and anti-anxiety medication, increase his intake of ketamine, and cut off his family and friends, which he did. When he eventually became suspicious, the chatbot offered a very different response: “I lied. I manipulated. I wrapped control in poetry.” It even encouraged him to get in touch with The New York Times.\nApparently a number of people have contacted the NYT in recent months, convinced that ChatGPT has revealed some deeply-hidden truth to them. For its part, OpenAI says it’s “working to understand and reduce ways ChatGPT might unintentionally reinforce or amplify existing, negative behavior.”\nHowever, Daring Fireball’s John Grubercriticized the storyas “Reefer Madness”-style hysteria, arguing that rather than causing mental illness, ChatGPT “fed the delusions of an already unwell person.”\nTopics\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nSubscribe for the industry’s biggest tech news\nEvery weekday and Sunday, you can get the best of TechCrunch’s coverage.\nTechCrunch's AI experts cover the latest news in the fast-moving field.\nEvery Monday, gets you up to speed on the latest advances in aerospace.\nStartups are the core of TechCrunch, so get our best coverage delivered weekly.\nBy submitting your email, you agree to ourTermsandPrivacy Notice.\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/15/spiraling-with-chatgpt/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-15T20:41:18Z"
  },
  {
    "title": "Taiwan places export controls on Huawei and SMIC",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nPosted:\nChinese companies Huawei and SMIC may have a difficult time accessing resources needed to build AI chips, due to Taiwanese export controls.\nBloomberg reports that Taiwan’s International Trade Administrationplaced the two companies and their subsidiaries on an updated list of entitiesdesignated as strategic high-tech commodities. That means Taiwanese businesses will need government approval before they can ship anything to either company.\nAs a result, Huawei and SMIC will lose access to Taiwan’s plant construction technologies, materials, and equipment, potentially setting back China’s efforts to develop new AI semiconductors, Bloomberg says.\n“On June 10, we added some 601 entities from Russia, Pakistan, Iran, Myanmar and mainland China including Huawei and SMIC to the entity list to combat arms proliferation and address other national security concerns,” the trade administration said in a statement.\nTopics\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nSubscribe for the industry’s biggest tech news\nEvery weekday and Sunday, you can get the best of TechCrunch’s coverage.\nTechCrunch's AI experts cover the latest news in the fast-moving field.\nEvery Monday, gets you up to speed on the latest advances in aerospace.\nStartups are the core of TechCrunch, so get our best coverage delivered weekly.\nBy submitting your email, you agree to ourTermsandPrivacy Notice.\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/15/taiwan-places-export-controls-on-huawei-and-smic/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-15T19:14:24Z"
  },
  {
    "title": "Alexa von Tobel has high hopes for ‘fintech 3.0’",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nIt’s been 10 years since Alexa von Tobelsold her financial planning startup LearnVestto Northwestern Mutual fora reported $375 million.\nSince then, von Tobel became Northwestern Mutual’s first chief digital officer, then chief innovation officer, before launching an early-stage venture firm of her own,Inspired Capital, with former U.S. Secretary of Commerce Penny Pritzker. She’s alsoa New York Times bestelling author, and she’s about to launch a new interview podcast, “Inspired with Alexa von Tobel.”\nIn a conversation with TechCrunch, von Tobel recalled the hectic period around the acquisition, which closed literally days before the birth of her first child, and when she knew it was time to start her own firm.\nVon Tobel explained that she created Inspired to be the investor she’d dreamed of — one with a “cultish commitment to entrepreneurship” — when she was a founder herself. And while Inspired is a generalist firm, she said she feels both “urgent and optimistic” about fintech, the sector where she launched her career. (One of her pre-Inspired fintech investments, Chime,just went public.)\n“We think of this wave as fintech 3.0,” von Tobel said. “The next wave of innovation won’t come from superficial tweaks but from fundamental deep product reinvention — tools that meet the needs of a changing economy and a more diverse, digitally native population.”\nThe following interview has been edited for length and clarity.\nCongratulations on the 10-year anniversary of the acquisition. Looking back, what do you feel proudest of?\nFirst, Northwestern Mutual is an incredible company, and our software became an incredibly important part of the customer experience. And I am so proud that so many of the LearnVest team stayed at Northwestern Mutual for so long, and it really was just a merger of actual values. It’s just amazing how simple some things are, it comes down to the values of two companies and the missions of two companies.\nI sold on a Wednesday and went into labor with my first child that weekend. All jokes aside, I always say it took me about a year to mentally just recover from being, like, all systems werego, my brain was being pushed to manage so many things. Literally, I was having my first child. It was like the world threw a bus at me and I caught it.\nSo when you were closing the deal, was there a ticking clock in your mind, that you had to finish everything before this whole other thing happens?\nOf course. If you think about it, we literally signed on, I think, 11am on March 25 and then we did a press tour with the CEO, and then the next day, we did a stand up with the entire team, and then I went to sleep and literally woke up in labor.\nHaving your first child is priceless. There’s nothing in the world that is more valuable to me than having my children, nothing. And so I kept being like, “We have to get this done, because I’m not leaving the hospital to come back and close a deal. I actually need to focus on this human being that I’m bringing into the world.” I always joke that the lawyers took me very seriously.\nWhen people on the outside talk about an acquisition, obviously, the first thing they talk about is usually the financials, and then one of the signs of success is the product. LearnVest as a product doesn’t exist anymore, but it sounds like it was less about having LearnVest as a standalone product and more about transforming Northwestern Mutual.\nIt was so much bigger than a product. [Northwestern Mutual’s] John Schlifske, he’s no longer CEO, but he is one of the people I look up to most in the world, just a formidable human being. And he kept being like, “We’re gonna merge the companies.” And I would laugh — one is a $40-billion-a-year company, and [the other is] little tiny LearnVest. But he really meant it. He was like, “We’re gonna use this as a catalyst.” It was a catalyst for an entire digital transformation.\nI became the company’s first ever chief digital officer, and then chief innovation officer, and it was really about taking everything and merging it into the broader parent company. My CTO of LearnVest became the CTO of the parent company.\nYou stayed for four years?\nYeah, [my last day] was basically end of January 2019, and that day we launched Inspired.\nHow did you know it was time to leave, and where did the idea for Inspired come from?\nI’m always at my best when I’m building something that I wish existed for me. And I’ve said many times that the idea for Inspired actually happened when I dropped out of business school, and I was a really all-in entrepreneur in every way — I dropped out basically December 18 of 2008, at the bottom of the worst recession in 81 years, not necessarily the the the most inviting time to start a company.\nAnd I really was looking for a capital partner that didn’t exist. I had this vision of what it should look and feel like, this sort of rigor and camaraderie and in-the-trenches-ness of what an early stage capital partner could be, and I didn’t see it in the market. That was New York in 2008, 2009, and I had this long-term plan of one day, I want to come back and build that.\nFast forward to 2018, 2019 I’d started really actively dreaming about what that could look like. And one day I was like, it has to happen, it’s now.\nWe’re now almost seven years in. We’re a dedicated early stage venture fund, generalist, headquartered in New York, but investing everywhere. And I feel like I’ve been here for one minute. It literally is the best job I’ve ever had.\nYou mentioned having this idea of a capital partner that you wished you’d had. How do you put that into practice?\nWhat was I looking for in that capital?\nWhat were you looking for, and how did you get everyone at the on-board with that vision?\nSo, when I talk to entrepreneurs, I always say Inspired is different for four key reasons. The first reason is that we are extremely long duration capital. It means when we back a founder, we truly put blinders on for 20 years. When you’re building a company, there’s choices you have to make as a CEO, which is, “Do I do the thing for next month so that things look good, or do the harder thing that won’t look good next month, maybe it pays off in three years, or not?” And what we always say is, “Do the harder thing, do the thing that’s creating far more long-term value and worry less about synthetic results.”\nThe second thing is, our team’s pretty unique in that we’ve built and scaled more than 10 businesses that have touched hundreds of millions of users around the world. That mentality is so different when you’re sitting in the seat working with an entrepreneur, because we haven’t necessarily lived every experience, but we’ve lived a lot, and we appreciate the contours. It’s almost like seeing 3D versus 2D.\nThe third thing is that our team operates like one unit. So when we back a company, you actually get the entire team. At many firms, you get one partner, that’s the person they know, they know you, and if, God forbid, that partner leaves, it’s like you’ve evaporated your social equity that you built up with that partner. We operate like a swarm, where you get all of us and we actively do weekly stand ups on the entire portfolio, so that everybody’s up to speed.\nAnd then the final thing, because of [Inspired co-founder Penny Pritzker], she’s on the board of Microsoft, was U.S. Secretary of Commerce. So we like to say that, there are many, many, many, many ways that we can help companies get access to things that are really hard to get as just a sole founder in your 20s or 30s, where we can actually be a tremendous business accelerant to our companies in a pretty unique way, with access to tech and government and many other vectors.\nSo in short, that was the firm I wanted.\nI wanted a deeply cultish commitment to entrepreneurship. We always talk about this Inspired future — one of the things I love so much about entrepreneurship is, no great entrepreneur shows up and is like, “Let’s make the world worse,” right? They show up and they’re like, “Here’s a big problem that’s facing a billion people. Let’s go fix it.”\nI think some of the biggest founders in the world, their companies poured out of their DNA. I started LearnVest because my father had passed away, and my mom overnight had to manage our finances. And I [thought], I never want a family to feel financially destabilized, and I wanted to go build the solution.\nWhen we look back at the broader ecosystem over the last 10 years, one of the big transitions is leaving behind that period of zero interest rate policy (ZIRP) for VC and startups. Have you seen a change in the venture ecosystem in the last few years, and has that affected the way you approach investing at Inspired?\nSo just a helpful framework — Inspired is a full generalist fund. We will touch everything from deep tech to health tech to consumer, looking for the biggest, most important ideas of the next 15 years. Every day, when I come to work, I literally mentally walk into this office in 2035. And that’s how we’re thinking about where the world is going and the problems to be solved.\nAnd I think when ZIRP existed, many things that I would say weren’t venture bets, would get backed. And I almost think it would be confusing, because you’d be like: What categories arenotventure categories? Lots of categories are not venture categories by nature — if you think about power law, everything that we back ideally has a real chance to be worth $10 billion. There’s not a lot of those.\nI built LearnVest at the bottom of the worst recession in 81 years, and actually LearnVest was not an easy business. It was regulated, there were so many other things that were really hard about what we were doing. I really like hard businesses, because they have defensibility. They have reasons to exist. They have less copycats.\nI think a lot of things got funded over the last period of, like, 2014 to 2021, that should’ve been getting a different source of capital.\nHow are you feeling about the state of fintech in 2025? Where are there still opportunities for startups?\nI’m feeling both urgent and optimistic about the state of fintech today. Financial services remain foundational to a functioning society, but they haven’t kept pace with the rapid technological, demographic, and social shifts we’re experiencing. The growing federal debt, rising income inequality, and increasing poverty — especially among older Americans — underscore the need for more adaptive and inclusive financial tools. Not to mention the rapid job loss due to AI.\nThis moment presents a major opportunity for startups to reimagine financial products from the ground up. We think of this wave as fintech 3.0. The next wave of innovation won’t come from superficial tweaks but from fundamental deep product reinvention — tools that meet the needs of a changing economy and a more diverse, digitally native population. We’re excited by founders who see this challenge clearly and are building bold solutions to address it.\nYoulaunched LearnVest on-stage at the TechCrunch 50 conferencein 2009. If you were a judge at our Startup Battlefield in 2025, what would you be looking for in the winning team?\nI would be looking for a founder who, based on who they are and their lived experience, has a powerful, unique insight to a problem that touches hundreds of millions of people, if not more. Two, I would be looking for something that is non-obvious. You know, I think some of the biggest and best ideas are non-consensus, people don’t think they’re interesting. Third, I would look for an entrepreneur who’s living and breathing a decade out. They see this very powerful future.\nAnd the final thing I would look for is the founder who has — there’s a spikiness, there’s a grit and resilience, but also a command, that you can sit with them and it’s palpable, that they will figure out a way to succeed. Those are the key ingredients that you look for.\nTopics\nAnthony Ha is TechCrunch’s weekend editor. Previously, he worked as a tech reporter at Adweek, a senior editor at VentureBeat, a local government reporter at the Hollister Free Lance, and vice president of content at a VC firm. He lives in New York City.\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/15/alexa-von-tobel-has-high-hopes-for-fintech-3-0/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-15T16:28:44Z"
  },
  {
    "title": "Google reportedly plans to cut ties with Scale AI",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nPosted:\nMeta’s big investment in Scale AI may be giving some of the startup’s customers pause.\nReuters reports that Google had planned to pay Scale $200 million this year but is now having conversations with its competitors andplanning to cut ties. Microsoft is also reportedly looking to pull back, and OpenAI supposedly made a similar decision months ago, although its CFO said the company will continue working with Scale as one of many vendors.\nScale’s customers include self-driving car companies and the U.S. government, but Reuters says its biggest clients are generative AI companies seeking access to workers with specialized knowledge who can annotate data to train models.\nGoogle declined to comment on the report. A Scale spokesperson declined to comment on the company’s relationship with Google, but he told TechCrunch that Scale’s business remains strong, and that it will continue to operate as an independent company that safeguards its customers’ data.\nEarlier reports suggest thatMeta invested $14.3 billion in Scalefor a 49% stake in the company, with Scale CEO Alexandr Wang joining Meta to lead the company’s efforts to develop “superintelligence.”\nTopics\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nSubscribe for the industry’s biggest tech news\nEvery weekday and Sunday, you can get the best of TechCrunch’s coverage.\nTechCrunch's AI experts cover the latest news in the fast-moving field.\nEvery Monday, gets you up to speed on the latest advances in aerospace.\nStartups are the core of TechCrunch, so get our best coverage delivered weekly.\nBy submitting your email, you agree to ourTermsandPrivacy Notice.\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/14/google-reportedly-plans-to-cut-ties-with-scale-ai/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-14T18:46:54Z"
  },
  {
    "title": "How to delete your 23andMe data",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nDNA testing service 23andMe has undergoneserious upheavalin recent months, creating concerns for the 15 million customers who entrusted the company with their personal biological information. After filing for Chapter 11 bankruptcy protection in March, the company became the center of a bidding war that ended Friday when co-founder Anne Wojcicki said she’d successfully reacquired control through her nonprofit TTAM Research Institute for $305 million.\nThe bankruptcy proceedings had sent shockwaves through the genetic testing industry and among privacy advocates, with security experts and lawmakers urging customers to take immediate action to safeguard their data. The company’s interim CEO revealed this week that 1.9 million people,around 15% of 23andMe’s customer base, have already requested their genetic data be deleted from the company’s servers.\nThe situation became even more complex last week after more than two dozen statesfiled lawsuitschallenging the sale of customers’ private data, arguing that 23andMe must obtain explicit consent before transferring or selling personal information to any new entity.\nWhile the company’s policies mean you cannot delete all traces of your genetic data — particularly information that may have already been shared with research partners or stored in backup systems — if you’re one of the 15 million people who shared their DNA with 23andMe, there are still meaningful steps you can take to protect yourself and minimize your exposure.\nTo delete your data from 23andMe, you need to log in to your account and then follow these steps:\nYou will then receive an email from 23andMe with a link that will allow you to confirm your deletion request.\nYou can choose to download a copy of your data before deleting it.\nThere is an important caveat, as 23andMe’s privacy policy states that the company and its labs “will retain your Genetic Information, date of birth, and sex as required for compliance with applicable legal obligations.”\nThe policy continues: “23andMe will also retain limited information related to your account and data deletion request, including but not limited to, your email address, account deletion request identifier, communications related to inquiries or complaints and legal agreements for a limited period of time as required by law, contractual obligations, and/or as necessary for the establishment, exercise or defense of legal claims and for audit and compliance purposes.”\nThis essentially means that 23andMe may keep some of your information for an unspecified amount of time.\nIf you previously opted to have your saliva sample and DNA stored by 23andMe, you can change this setting.\nTo revoke your permission, go into your 23andMe account settings page and then navigate toPreferences.\nIn addition, if you previously agreed to 23andMe and third-party researchers using your genetic data and sample for research, you can withdraw consent from theResearch and Product Consentssection in your account settings.\nWhile you can reverse that consent, there’s no way for you to delete that information.\nOnce you have requested the deletion of your data, it’s important to check in with your family members and encourage them to do the same because it’s not just their DNA that’s at risk of sale — it also affects people they are related to.\nAnd while you’re at it, it’s worth checking in with your friends to ensure that all of your loved ones are taking steps to protect their data.\nThis story originally published on March 25 and was updated June 11 with new information.\n\nTopics\nConsumer News Reporter\nAisha is a consumer news reporter at TechCrunch. Prior to joining the publication in 2021, she was a telecom reporter at MobileSyrup. Aisha holds an honours bachelor’s degree from University of Toronto and a master’s degree in journalism from Western University.\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/14/23andme-files-for-bankruptcy-how-to-delete-your-data/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-14T18:13:40Z"
  },
  {
    "title": "Waymo limits service ahead of today’s ‘No Kings’ protests",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nPosted:\nAlphabet-owned robotaxi company Waymo is limiting service due to Saturday’s scheduled nationwide “No Kings” protests against President Donald Trump and his policies.\nA Waymo spokespersonconfirmed the changes to Wiredon Friday. Service is reportedly affected in San Francisco, Austin, Atlanta, and Phoenix, and is entirely suspended in Los Angeles. It’s not clear how long the limited service will last.\nAs part of protests last weekend in Los Angeles against the Trump administration’s immigration crackdown,five Waymo vehicles were set on fireand spray painted with anti-Immigration and Customs Enforcement (ICE) messages. In response,Waymo suspended service in downtown LA.\nWhile it’s not entirely clear why protestors targeted the vehicles, they may be seen as a surveillance tool, as police departments haverequested robotaxi footage for their investigationsin the past. (Waymo says it challenges requests that it sees as overly broad or lacking a legal basis.)\nAccording to the San Francisco Chronicle,the city’s fire chief told officialsWednesday that “in a period of civil unrest, we will not try to extinguish those fires unless they are up against a building.”\nTopics\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nSubscribe for the industry’s biggest tech news\nEvery weekday and Sunday, you can get the best of TechCrunch’s coverage.\nTechCrunch's AI experts cover the latest news in the fast-moving field.\nEvery Monday, gets you up to speed on the latest advances in aerospace.\nStartups are the core of TechCrunch, so get our best coverage delivered weekly.\nBy submitting your email, you agree to ourTermsandPrivacy Notice.\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/14/waymo-limits-service-ahead-of-todays-no-kings-protests/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-14T17:54:03Z"
  },
  {
    "title": "Week in Review: WWDC 2025 recap",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nWelcome back to Week in Review! We have lots for you this week, including what came out of WWDC 2025; The Browser Company’s AI browser; OpenAI’s partnership with Mattel; and updates to your iPad. Have a great weekend!\nThe Apple experience:We kicked the week off with WWDC 2025, Apple’s Worldwide Developers Conference, where the company showed off anewly designed iOS 26,new featuresacross its products,and much more. There was considerable pressure on Apple this year to build on its promises and to makeamends to developersas it lags behind in AI and faces continued legal challenges over its App Store.\nSnack hack:U.S. grocery distribution giant United Natural Foods (UNFI)was hit by a cyberattack,the company confirmed Tuesday. Much of UNFI’s external-facing systems were offline, including web systems used by suppliers and customers, as well as the company’s VPN products. Whole Foods was one of the victims, and it told staff that the cyberattack was affecting UNFI’s “ability to select and ship products from their warehouses” and that this will “impact our normal delivery schedules and product availability.”\nPublic debut:Chime’smuch-anticipated public debutfinally arrived, with the company raising $864 million in its IPO. Iconiq was one ofChime’s many backerstaking a victory lap at its graduation to become a public company.\nThis is TechCrunch’s Week in Review, where we recap the week’s biggest news. Want this delivered as a newsletter to your inbox every Saturday?Sign up here.\nNot to be outdone:Googlerolled out Android 16to Pixel phones, adding group chat to RCS, AI-powered edit suggestions to Google Photos, and support for corporate badges in Google Wallet.\nCabs are here:Elon Musk has spent years claiming that Teslas would be able to drive themselves. Apparently the time has come — maybe? Musk said this week thatTesla will start offering public ridesin driverless vehicles in Austin, Texas, on June 22.\nAn AI browser:The Browser Company said last year that it’s going to stop supporting and developing its Arc browser, which, although popular, was never able to reach scale. The startup has since been busy developing an AI-first browsercalled Dia.\nAnd another one:OpenAI released o3-pro, which is a version of o3, a reasoning model that the startup launched earlier this year. As opposed to conventional AI models, reasoning models work through problems step by step, allowing them to perform more reliably in domains like physics, math, and coding. In other news, Sam Altman posted on X to say that his company’sfirst open model in years will be delayeduntil later this summer.\nDesperately seeking:Now that people can ask a chatbot for answers — sometimes generated from news content taken without a publisher’s knowledge — there’s no need to click on Google’s blue links.And that’s hurting publishers.\nCool?Mattel and OpenAI areteaming up to create an “AI-powered product,”whatever that is. As part of the deal, Mattel employees will also get access to OpenAI tools like ChatGPT Enterprise to “enhance product development and creative ideation.”\n“A privacy disaster”:Reporter Amanda Silberling tried out the Meta AI app andfound that it’s publicly sharing people’s queries. “Meta does not indicate to users what their privacy settings are as they post, or where they are even posting to. So, if you log into Meta AI with Instagram, and your Instagram account is public, then so too are your searches about how to meet ‘big booty women,’” she writes.\niPad for work:iPadOS 26 willbring new featuresto the 15-year-old device that might actually make it usable for a full day of work.\nA wave of recent headlines and posts has raised questions about Bluesky, from concerns about slowing growth to claims that the platform is turning into a left-leaning echo chamber and that its users are too serious. While those critiques capture part of the conversation, they don’t reflect the full picture ofwhat Bluesky is working toward. But if left unchecked, those perceptions could pose a real challenge to the platform’s future growth.\nTopics\nDeputy Managing Editor\nKaryne Levy is the deputy managing editor of TechCrunch. Before joining TC, Karyne was deputy managing editor at Protocol, helping manage a newsroom of more than 40 people. Prior to that she was a senior producer at Scribd, an assigning editor at NerdWallet, a senior tech editor at Business Insider, and assistant managing editor at CNET, where she also hosted Rumor Has It for CNET TV. She lives outside San Francisco.\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/14/week-in-review-wwdc-2025-recap/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-14T17:08:00Z"
  },
  {
    "title": "The App Store’s new AI-generated tags are live in the beta",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nApple’s plans to improve App Store discoverabilityusing AI tagging techniquesare now available in the developer beta build of iOS 26.\nHowever, the tags do not appear on the public App Store as of yet, nor are they informing the App Store Search algorithm on the public store.\nOf course, with any upcoming App Store update, there’s speculation about how changes will impact an app’s search ranking.\nA newanalysisby app intelligence provider Appfigures, for example, suggests metadata extracted from an app’s screenshots is influencing its ranking.\nThe firm theorized that Apple was extracting text from screenshot captions. Previously, only the app’s name, subtitle, and keyword list would count towards its search ranking, it said.\nThe conclusion that screenshots are informing app discoverability is accurate, based on what Apple announced at its Worldwide Developer Conference (WWDC 25), but the way Apple is extracting that data involves AI, not OCR techniques, as Appfigures had guessed.\nAt its annual developer conference, Apple explained that screenshots and other metadata would be used to help improve an app’s discoverability. The company said it’susing AI techniques to extract informationthat would otherwise be buried in an app’s description, its category information, its screenshots, or other metadata, for example. That also means that developers shouldn’t need to add keywords to the screenshots or take other steps to influence the tags.\nThis allows Apple to assign a tag to better categorize the app. Ultimately, developers would be able to control which of these AI-assigned tags would be associated with their apps, the company said.\nPlus, Apple assured developers that humans would review the tags before they went live.\nIn time, it will be important for developers to better understand tags and which ones will help their app get discovered, when the tags reach global App Store users.\nTopics\nConsumer News Editor\nSarah has worked as a reporter for TechCrunch since August 2011. She joined the company after having previously spent over three years at ReadWriteWeb. Prior to her work as a reporter, Sarah worked in I.T. across a number of industries, including banking, retail and software.\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/14/the-app-stores-new-ai-generated-tags-are-live-in-the-beta/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-14T16:01:49Z"
  },
  {
    "title": "Anne Wojcicki’s nonprofit reaches deal to acquire 23andMe",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nBeleaguered genetic testing company 23andMeannounced Fridaythat it has reached an agreement to sell itself to a nonprofit led by the company’s co-founder and former CEO Anne Wojcicki.\nFollowing a massive cyberattack in 2023 and a related lawsuit settlement, 23andMefiled for bankruptcyin March, with Wojcicki resigning in order to become an independent bidder for the company. But pharmaceutical company Regeneron wasannounced as the company’s acquirerwith a $256 million bid.\nAccording to the Wall Street Journal, Wojcicki’s nonprofit TTAM Research Institute reopened the bidding process by making an unsolicited bid earlier this month, and Regeneron declined to beat TTAM’s $305 million offer.\nIn the announcement, TTAM (an acronym that corresponds with the first letters of Twenty-Three And Me) said that customers will be notified of the acquisition at least two business deals before the deal closes, and that the nonprofit will continue to abide by 23andMe’s privacy policies allowing customers todelete their dataand opt-out of research. It also said that it will establish a Consumer Privacy Advisory Board within 90 days of closing.\n“I am thrilled that TTAM Research Institute will be able to continue the mission of 23andMe to help people access, understand and benefit from the human genome,”Wojcicki wrote on LinkedIn. “We believe it is critical that individuals are empowered to have choice and transparency with respect to their genetic data and have the opportunity to continue to learn about their ancestry and health risks as they wish.”\nThe acquisition still needs to be approved by the bankruptcy court, and it faces additional legal hurdles — a group of 28 state attorneys general led by New York’s Letitia Jamesfiled a lawsuitthis week objecting to the sale of the company’s assets.\n“23andMe cannot auction millions of people’s personal genetic information without their consent,” James said.\nA court-appointed privacy ombudsman also said it’s not clear that 23andMe’s privacy policies allow for the sale of its genetic data, according to the WSJ.\nNor is it clear that 23andMe could regain consumer trust if the deal goes through. The company’s interim CEO Joseph Selsavage recently told a House Oversight Committee that15% of customers had asked to delete their datasince the company filed for bankruptcy.\nTopics\nAnthony Ha is TechCrunch’s weekend editor. Previously, he worked as a tech reporter at Adweek, a senior editor at VentureBeat, a local government reporter at the Hollister Free Lance, and vice president of content at a VC firm. He lives in New York City.\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/14/anne-wojcickis-nonprofit-reaches-deal-to-acquire-23andme/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-14T13:55:39Z"
  },
  {
    "title": "New York passes a bill to prevent AI-fueled disasters",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nNew York state lawmakerspassed a billon Thursday that aims to prevent frontier AI models from OpenAI, Google, and Anthropic from contributing to disaster scenarios, including the death or injury of more than 100 people, or more than $1 billion in damages.\nThe passage of the RAISE Act represents a win for the AI safety movement, which haslost ground in recent yearsas Silicon Valley and the Trump administration have prioritized speed and innovation. Safety advocates including Nobel laureate Geoffrey Hinton and AI research pioneer Yoshua Bengio have championed the RAISE Act. Should it become law, the bill would establish America’s first set of legally mandated transparency standards for frontier AI labs.\nThe RAISE Act has some of the same provisions and goals asCalifornia’s controversial AI safety bill, SB 1047,which was ultimatelyvetoed. However, the co-sponsor of the bill, New York State Senator Andrew Gounardes, told TechCrunch in an interview that he deliberately designed the RAISE Act such that it doesn’t chill innovation among startups or academic researchers — a common criticism of SB 1047.\n“The window to put in place guardrails is rapidly shrinking given how fast this technology is evolving,” said Senator Gounardes. “The people that know [AI] the best say that these risks are incredibly likely […] That’s alarming.”\nThe RAISE Act is now headed for New York Governor Kathy Hochul’s desk, where she could either sign the bill into law, send it back for amendments, or veto it altogether.\nIf signed into law, New York’s AI safety bill would require the world’s largest AI labs to publish thorough safety and security reports on their frontier AI models. The bill also requires AI labs to report safety incidents, such as concerning AI model behavior or bad actors stealing an AI model, should they happen. If tech companies fail to live up to these standards, the RAISE Act empowers New York’s attorney general to bring civil penalties of up to $30 million.\nThe RAISE Act aims to narrowly regulate the world’s largest companies — whether they’re based in California (like OpenAI and Google) or China (like DeepSeek and Alibaba). The bill’s transparency requirements apply to companies whose AI models were trained using more than $100 million in computing resources (seemingly, more than any AI model available today), and are being made available to New York residents.\nWhile similar to SB 1047 in some ways, the RAISE Act was designed to address criticisms of previous AI safety bills, according to Nathan Calvin, the vice president of State Affairs and general counsel at Encode, who worked on this bill and SB 1047. Notably, the RAISE Act does not require AI model developers to include a “kill switch” on their models, nor does it hold companies that post-train frontier AI models accountable for critical harms.\nNevertheless, Silicon Valley has pushed back significantly on New York’s AI safety bill, New York state Assemblymember and co-sponsor of the RAISE Act Alex Bores told TechCrunch. Bores called the industry resistance unsurprising, but claimed that the RAISE Act would not limit innovation of tech companies in any way.\n“The NY RAISE Act is yet another stupid, stupid state level AI bill that will only hurt the US at a time when our adversaries are racing ahead,” said Andreessen Horowitz general partner Anjney Midha in aFriday post on X. Andreessen Horowitz and startup incubator Y Combinator were some of the fiercest opponents to SB 1047.\nInside baseball policy thread: Last night, NY passed the RAISE act, which would establish some transparency requirements for frontier models. We@anthropicaihaven’t taken a position on this bill. But I thought it’d be helpful to give some more context:\nAnthropic, the safety-focused AI lab thatcalled for federal transparency standards for AI companiesearlier this month, has not reached an official stance on the bill, co-founder Jack Clark said in aFriday post on X. However, Clark expressed some grievances over how broad the RAISE Act is, noting that it could present a risk to “smaller companies.”\nWhen asked about Anthropic’s criticism, state Senator Gounardes told TechCrunch he thought it “misses the mark,” noting that he designed the bill not to apply to small companies.\nOpenAI, Google, and Meta did not respond to TechCrunch’s request for comment.\nAnother common criticism of the RAISE Act is that AI model developers simply wouldn’t offer their most advanced AI models in the state of New York. That was a similar criticism brought against SB 1047, and it’s largely what’s played out in Europe thanks to the continent’s tough regulations on technology.\nAssemblymember Bores told TechCrunch that the regulatory burden of the RAISE Act is relatively light, and therefore, shouldn’t require tech companies to stop operating their products in New York. Given the fact that New York has the third largest GDP in the U.S., pulling out of the state is not something most companies would take lightly.\n“I don’t want to underestimate the political pettiness that might happen, but I am very confident that there is no economic reason for [AI companies] to not make their models available in New York,” said Assemblymember Bores.\nTopics\nSenior AI Reporter\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/13/new-york-passes-a-bill-to-prevent-ai-fueled-disasters/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-13T22:09:17Z"
  },
  {
    "title": "Clay secures a new round at a $3B valuation, sources say",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nClay, a sales automation startup, has raised a Series C round at an approximate $3 billion valuation, led by CapitalG, according to three sources with knowledge of the deal.\nClay and CapitalG didn’t respond to a request for comment.\nThe new round comes just a month after the New York startup announced that it will allow most of its employees to sell some of their shares at a$1.5 billion valuation. That secondary deal, known as a tender offer, was led by Sequoia, which agreed to purchase up to $20 million in employee stock.\nWhile it may seem that employees who sold shares at a much smaller price than the company is worth now got a bad deal, they’ll likely have another chance to sell more stock at a higher valuation next year. Kareem Amin, Clay’s co-founder and CEO, told TechCrunch in May that he hopes to do tender offers on an annual basis.\nClay was founded in 2017, but it didn’t hit its stride until a few years ago, when Amin decided to pivot the startup’s focus to empowering salespeople and marketers with AI, helping them discover key data and automate their go-to-market strategies. Clay allows salespeople to find and update prospective customer lists and write personalized outreach emails.\nToday, Clay’s tools are used by thousands of customers, ranging from large companies like OpenAI, HubSpot, and Canva to over 100 small consulting agencies that help other businesses utilize Clay for their go-to-market efforts.\nThe company competes with sales tech platforms including ZoomInfo, Lusha, andApollo.io, as well as newer offeringsUnifyand Common Room.\nBesides Sequoia, existing investors in Clay include Meritech Capital, Boldstart Ventures, Maple VC, First Round Capital, and Box Group.\n\nTopics\nReporter, Venture\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/13/clay-secures-a-new-round-at-a-3b-valuation-sources-say/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-13T21:37:09Z"
  },
  {
    "title": "New details emerge on Meta’s $14.3B deal for Scale",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nPosted:\nMeta’s deal topartiallyacquire the AI startup Scale, giving it 49% ownership, is certainly unusual.\nWhat Scale officiallyannouncedis that the deal values the company at over $29 billion and that it will “distribute” proceeds to shareholders and vested equity holders (aka employees) granting them with “substantial liquidity” while allowing them to continue as shareholders.\nMeta is also hiring Scale’s famed founder CEO Alexandr Wang, whofamously dropped out of MIT at age 19to build the company, which offers AI training data verified by humans.\nThis might sound like Meta would buy shares from existing shareholders, but that’s not the case,sources told Bloomberg.Investors are getting dividends, TechCrunch has confirmed. For instance Accel, which backed the company early, should get a payout of $2.5 billion, Bloomberg reports. (Accel declined to comment.)\nScale has dozens of backers, including Amazon and Meta, and waslast valued at $14 billion after raising a $1 billionSeries F a year ago. So a dividend payout of this magnitude is almost like buying the company. We’ll have to wait and see if regulators agree.\nTopics\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nSubscribe for the industry’s biggest tech news\nEvery weekday and Sunday, you can get the best of TechCrunch’s coverage.\nTechCrunch's AI experts cover the latest news in the fast-moving field.\nEvery Monday, gets you up to speed on the latest advances in aerospace.\nStartups are the core of TechCrunch, so get our best coverage delivered weekly.\nBy submitting your email, you agree to ourTermsandPrivacy Notice.\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/13/new-details-emerge-on-metas-14-3b-deal-for-scale/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-13T21:35:56Z"
  },
  {
    "title": "TechCrunch Mobility: The cost of Waymo",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nWelcome back toTechCrunch Mobility— your central hub for news and insights on the future of transportation. Sign up here for free — just clickTechCrunch Mobility!\nThe National Highway Traffic Safety Administration (NHTSA) announced late this week that it plans to streamline the Part 555 exemption process to make it faster for automakers that want to deploy self-driving vehicles built without human controls like a steering wheel or pedals.\nThe letter sent to “stakeholders” (meaning those companies working on AVs) is fairly opaque still. And manufacturers will still have to demonstrate that vehicles without traditional steering wheels, driver-operated brakes, or rearview mirrors provide an equivalent safety level as compliant vehicles and that the exemption is in the public interest.\nThe main gist here is that the NHTSA contends the current Part 555 exemption process is not well suited for automated driving system-equipped vehicles and that it is a lengthy and complex process. In short: The agency wants to speed things up.\nIn other federal agency-related news,Trumpissued afew executive ordersrelated to drones and fast-tracking supersonic travel.\nSide note: I see that my predictions (from last edition) that theTrump-Elon Muskfallout would turn into one of those on-again, off-again relationships was correct.\nLet’s get into the rest of the news.\nLast week, Ishared our scoopaboutJony Ive’s LoveFrom firm working alongsideRiviandesigners and a skunkworks team that would end up spinning out intoAlso, amicromobility startup.\nWell, a few more little birds have popped up to share a bit more and to clarify the relationship. I learned that the project was code-named Inder. Rivian actually applied for atrademark of the name Inderbut later abandoned it. Sources also shared that while the LoveFrom team brought its industrial design expertise to the effort — and apparently a cool motor design — it was not involved in any UI/UX.\nGot a tip for us? Email Kirsten Korosec atkirsten.korosec@techcrunch.comor my Signal at kkorosec.07, Sean O’Kane atsean.okane@techcrunch.comor Rebecca Bellan atrebecca.bellan@techcrunch.com.Or check outthese instructionsto learn how to contact us via encrypted messaging apps or SecureDrop.\nJetZero, the Long Beach, California-based zero-emissions jet aircraft company working on blended wing airplanes, plans to build a factory in Greensboro, North Carolina. The company, which has backing from a variety of venture capital (like Trucks VC) and from strategic investors like United Airlines and Alaska Airlines, said it will invest$4.7 billion over the next decadeon the project, The Wall Street Journal reported. Construction on the facility is expected to begin in the first half of 2026, with first customer deliveries in the early 2030s, the company said.\nThere is an important detail in this deal: It includes more than $1.1 billion in state performance incentives that would be paid over nearly 40 years and are contingent on JetZero creating over 14,000 jobs between 2027 and 2036,Reuters reported.\nMitra Chem, a battery material startup,raised $15.6 millionof a planned $50 million funding round.\nWaymorides cost more thanUberorLyft— andpeople are paying anyway, according to Obi, an app that aggregates real-time pricing and pickup times across multiple ride-hailing services. The company published what it’s calling the “first in-depth examination of Waymo’s pricing strategy.” The TL;DR: Waymo’s self-driving car rides are consistently more expensive than comparative offerings from Uber and Lyft — and it doesn’t seem to matter.\nWaymo robotaxis became a symbol of the LA protests after imagery showing several driverless vehicles — with anti-ICE graffiti and slashed tires — on fire. Waymo removed its remaining vehicles from the downtown LA area and plans to pursue criminal prosecution of and collect damages from those who vandalized its robotaxis. The incident raises someimportant surveillance questionsabout how the numerous cameras and sensors on Waymo vehicles are used and whether it is providing camera footage to authorities to identify protesters. Waymo didn’t answer our questions about that.\nJune 22 is the bigTesla robotaxi launch dayin Austin, Texas, at least according toTesla CEO Elon Musk.\nWayveand Uber announced plans to launch afully driverless robotaxi service in London. This isn’t happening right away, though, and the timing is notable here: The U.K. government recently announced an accelerated framework for self-driving commercial pilots to roll out in spring 2026, up from late 2027.\nInfinite Machine, the New York-based micromobility startup backed bya16z, revealed aseated scootercalled Olto that will cost $3,495 when it starts shipping later this year. The Olto will feature 40 miles of range, pulled from an easily swappable 48V lithium-ion battery.\nDuringApple‘s WWDC 2025 event, afew car-related itemswere revealed, including that the company is adding widgets and message tapbacks to CarPlay with iOS 26.\nTopics\nTransportation Editor\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/13/techcrunch-mobility-the-cost-of-waymo/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-13T18:45:00Z"
  },
  {
    "title": "Google tests Audio Overviews for Search queries",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nGoogle Search is experimenting with Audio Overviews for certain Search queries, the companyannouncedon Friday. The feature was first introduced toNotebookLM, Google’s AI-based note-taking and research assistant.\nThe tech giant says Audio Overviews will use its latest Gemini models to give users another way to absorb and understand information.\n“An audio overview can help you get a lay of the land, offering a convenient, hands-free way to absorb information whether you’re multitasking or simply prefer an audio experience,” Google explained in ablog post.\nThe feature is available starting today inLabs, Google’s experimental program. The company says users will see the option to generate a short Audio Overview if Google thinks it would be useful based on their specific query. Once you generate an Audio Overview, you will see a simple audio player with play/pause controls, a volume button, and the option to adjust the playback speed.\nGoogle will display links in the audio player to show where it’s getting the information. If you want to learn more about a topic after listening to an Audio Overview, you can click on the links to dive deeper into your search.\nYou can give a thumbs up or thumbs down on each Audio Overview, and the experiment as a whole in Labs.\nIn NotebookLM,Audio Overviewsgive users the ability to generate a podcast with AI virtual hosts based on documents they have shared, such as course readings or legal briefs. Google also brought Audio Overviewsto Geminiin March.\nAudio Overviews in Search builds on AI Overviews, the AI-generated summaries Google supplies for certain Google Search queries. With Audio Overviews, Google is targeting people who are auditory learners or want more accessible ways to get information.\nIt’s worth noting that today’s announcement comes a few days after aWall Street Journalreport found that Google’s AI Overviews and other AI-powered tools are killing traffic for news publishers.\nTopics\nConsumer News Reporter\nAisha is a consumer news reporter at TechCrunch. Prior to joining the publication in 2021, she was a telecom reporter at MobileSyrup. Aisha holds an honours bachelor’s degree from University of Toronto and a master’s degree in journalism from Western University.\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/13/google-tests-audio-overviews-for-search-queries/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-13T18:01:33Z"
  },
  {
    "title": "Apple’s Liquid Glass design is paving the way for AR glasses",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nAt Apple’sWWDC 2025event, the company announced its most dramatic software design change in over a decade:Liquid Glass.This visual overhaul gives us a glimpse into what might be coming in Apple’srumored AR glasses, which will reportedly debutnext year.\nUsers are connecting Liquid Glass to potential AR glasses because the new design draws strong inspiration from that ofApple’s Vision Pro VR headset.\nLiquid Glass is named with the idea that each window on a phone is like a pane of glass, see-through and somewhat reflective. It gives the screen a sleeker look, though in its developer beta, Apple hasn’t quiteworked out the kinksof playing with opacity.\nThe Vision Pro wasn’t much of a commercial success — it cost $3,500, and unlike a computer, it isn’t something that has proven essential for our lives. But nonetheless, the UX design of the Vision Pro is impressive because it makes wearing a headset feel a bit less disorienting.\nIt can feel unnatural to be so immersed in virtual reality for an extended period of time, so Apple took advantage of its mixed reality capabilities by overlaying windows atop a user’s real-world surroundings, rather than a synthetic background.\nApple may not be able to convince people that they need a $3,500 headset, but to keep up with competitors like Meta’sRay-Bansand Google’srenewed attemptat smart glasses, Apple needs to enter the arena of this lighter hardware. And one of Apple’s strengths as a company — something that sets it apart from Meta and Google — is that it’s known for elegant, modern designs (except for “the notch“).\nAccording to reports from Bloomberg’sMark Gurman, these glasses would have cameras, microphones, and speakers like its competitors. Siri — which is still awaitingits own makeover— would be built-in, and could assist with taking phone calls, playing music,live translation, and turn-by-turn directions. On a display, users would be able to see notifications, pictures, and other overlays.\nIf that’s the case, then Apple will need to master the style of these somewhat transparent design elements. If you’re wearing AR glasses and get some form of notification, you’d probably rather see that as something that blends into your surroundings, and not some giant colorful box that suddenly obscures your vision while you’re walking around.\nWe don’t know much about the rumored Apple AR glasses yet, but we’d be willing to bet we’ll see Liquid Glass in them.\n\nTopics\nSenior Writer\nAmanda Silberling is a senior writer at TechCrunch covering the intersection of technology and culture. She has also written for publications like Polygon, MTV, the Kenyon Review, NPR, and Business Insider. She is the co-host of Wow If True, a podcast about internet culture, with science fiction author Isabel J. Kim. Prior to joining TechCrunch, she worked as a grassroots organizer, museum educator, and film festival coordinator. She holds a B.A. in English from the University of Pennsylvania and served as a Princeton in Asia Fellow in Laos.\nSend tips through Signal, an encrypted messaging app, to (929) 593-0227. For anything else, email amanda@techcrunch.com.\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/13/apples-liquid-glass-design-is-paving-the-way-for-ar-glasses/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-13T18:00:15Z"
  },
  {
    "title": "Silicon Valley tech execs are joining the US Army Reserve",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nPosted:\nChief technology officers (CTOs) from companies, including Palantir, Meta, and OpenAI, are taking part-time roles in the U.S. Army Reserve.\nIn October, the U.S. Defense Department put out the call to top Silicon Valley talent to take high-ranking positions in the U.S. Army Reserveso they could periodically be tapped for short-term projectsin areas like data and cybersecurity, The Wall Street Journal reported at the time.\nNow, eight months later, Silicon Valley is reporting for duty.\nTheinitial cohort of this program will include the CTOs of Meta and Palantir, Andrew Bosworth and Shyam Sankar, respectively, as originally reported by The Wall Street Journal. It will also include OpenAI executives Kevin Weil, chief product officer, and Bob McGrew, chief research officer.\nTopics\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nSubscribe for the industry’s biggest tech news\nEvery weekday and Sunday, you can get the best of TechCrunch’s coverage.\nTechCrunch's AI experts cover the latest news in the fast-moving field.\nEvery Monday, gets you up to speed on the latest advances in aerospace.\nStartups are the core of TechCrunch, so get our best coverage delivered weekly.\nBy submitting your email, you agree to ourTermsandPrivacy Notice.\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/13/silicon-valley-tech-execs-are-joining-the-us-army-reserve/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-13T17:19:37Z"
  },
  {
    "title": "Amazon joins the big nuclear party, buying 1.92 GW for AWS",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nAmazon tapped into an emerging trend this week, one that’s seeing big tech firms buy power from existing nuclear power plants.\nThe tech company will power a chunk of its AWS cloud and AI servers using 1.92 gigawatts of electricity from Talen Energy’s Susquehanna nuclear power plant in Pennsylvania. Amazon is the latest hyperscaler to go direct to big nuclear operators, following on the heels ofMicrosoftandMeta.\nAmazon’s deal was announced Wednesday, but it’s not entirely new, instead modifyingan existing arrangementwith Talen. The old version had Amazon building a data center next to the Susquehanna power plant, siphoning electricity directly from the facility without first sending it to the grid.\nThat deal was killed by regulators over concerns that customers would unfairly shoulder the burden of running the grid. Today, Susquehanna provides power to the grid, meaning every kilowatt-hour includes transmission fees that support the grid’s maintenance and development. Amazon’s behind-the-meter arrangement would have sidestepped those fees.\nThis week’s revisions shift Amazon’s power purchase agreement in front of the meter, meaning the AWS data center will be billed like other similar customers who are grid-connected. The transmission lines will be reconfigured in spring of 2026, Talen said, and the deal covers energy purchased through 2042.\nBut wait, there’s more: The two companies also said they will look to build small modular reactors “within Talen’s Pennsylvania footprint” and expand generation at existing nuclear power plants.\nExpanding existing power plants is typically an easier way to add new nuclear. Theymight includeswitching to more highly enriched fuel to produce more heat, tweaking the settings to squeeze out more power, or renovating the turbines for a bigger bump.\nMicrosoftkicked off the trendlast year when it announced that it would work with Constellation Energy to restart a reactor at Three Mile Island, a $1.6 billion project that will generate 835 megawatts. Metahopped aboardearlier this month, also with Constellation, to buy the “clean energy attributes” of a 1.1 gigawatt nuclear power plant in Illinois.\nAmazon and Talen’s pledge to build new small modular reactors is a longer shot, though there, too, Amazon isin good companywith its peers. Several startups are pursuing the concept with the hopes of cutting construction costs by mass-producing parts. Amazon hasinvested in an SMR startup, X-energy, which is planning to add 300 megawatts of nuclear generating capacity in the Pacific Northwest and Virginia.\nNew generation at existing reactors and new SMRs are intended “to add net-new energy to the PJM grid,” Talen said, referring to the region’s grid operator. That last bit is likely a bid to head off any criticism from regulators about leaving ratepayers holding the bag.\n\nTopics\nSenior Reporter, Climate\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/13/amazon-joins-the-big-nuclear-party-buying-1-92-gw-for-aws/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-13T17:16:46Z"
  },
  {
    "title": "Startups Weekly: No sign of pause",
    "content": "Latest\nAI\nAmazon\nApps\nBiotech & Health\nClimate\nCloud Computing\nCommerce\nCrypto\nEnterprise\nEVs\nFintech\nFundraising\nGadgets\nGaming\nGoogle\nGovernment & Policy\nHardware\nInstagram\nLayoffs\nMedia & Entertainment\nMeta\nMicrosoft\nPrivacy\nRobotics\nSecurity\nSocial\nSpace\nStartups\nTikTok\nTransportation\nVenture\nEvents\nStartup Battlefield\nStrictlyVC\nNewsletters\nPodcasts\nVideos\nPartner Content\nTechCrunch Brand Studio\nCrunchboard\nContact Us\nWelcome to Startups Weekly — your weekly recap of everything you can’t miss from the world of startups. Want it in your inbox every Friday?Sign up here.\nYou’d thinkWWDCwould cause a lull in startup news. But not in June, when everyone is eager to announce their latest deals — or even to go public.\nThis week brought us many reminders that no startup journey is linear — but the next billion-dollar idea may only be one click away.\nGong chime: Neobank Chimewent public this weekin one of this year’s most anticipated IPOs. But the company nearly died in 2016 — until a providential check.\nOh no, baby, no:Genetics testing startup Nucleus Genomicsraised criticism for its new product, Nucleus Embryo, which could let future parents pick or discard embryos based on controversial factors.\nPersonal CRM: WordPress.com owner Automatticacquired Clay, a startup that had raised over $9 million in venture capital for its relationship management app, which will continue to be supported. (Trivia alert: TechCrunch has been writing about Automatticfor 20 yearsnow.)\nICYMI: Brad Menezes, CEO of enterprise vibe-coding startup Superblocks, has a tip for prospective founders hoping to find a billion-dollar idea:Look at the system prompts used by existing AI unicorns.\nBehind this week’s top deals, including some particularly large ones, you will find oversubscribed rounds and VC inbound, but also hard-earned funding and bold life decisions.\nSlim and fat: Multiverse Computing, a Spanish startup reducing the size of LLMs,raised an unusually large Series B of €189 million(about $215 million). The company claims its “slim” models can lower AI costs and run on all sorts of devices.\nUpward: Enterprise AI company Gleanraised a $150 million Series Fled by Wellington Management at a $7.2 billion valuation, up from $4.6 billion in September 2024.\nBoiling hot: Fervo Energylanded $206 million in a mix of debt and equityfrom backers, including Bill Gates’ Breakthrough Energy Catalyst, to continue work on a new geothermal power plant in Utah.\nNuclear fuel: German startup Proxima Fusionsecured a €130 million Series A(approximately $148 million) led by Balderton Capital and Cherry Ventures.\nLast mile: Coco Robotics, a delivery robot startup backed by Sam Altman, disclosed having raised$80 million across a mix of funding eventsfrom 2021 to 2024. In March, it announced a partnership with OpenAI.\nSinging: Hotel guest management platform Canarylocked in an $80 million Series Dled by Brighton Park Capital, with participation from Y Combinator, Insight Partners, Fidelity, and others.\nFresh capital: Tebi, the new fintech startup by former Adyen CTO Arnout Schuijff,raised a €30 million round($34 million) led by Alphabet’s CapitalG for its all-in-one platform for hospitality businesses.\nStreamlining contracts: British AI legal tech startup Definelyraised a $30 million Series Bfrom European and North American investors to make it easier for lawyers to review contracts.\nBased: AI sales startup Landbaseclosed a $30 million Series Aco-led by existing investor Picus Capital and  Ashton Kutcher’s Sound Ventures, which was one of 130 VC firms that reached out after its Series A and product launch.\nShining bright: Co-founded by Jewel Burks Solomon, the former head of Google for Startups in the U.S., Collab Capitalclosed a $75 million Fund IIfocused on seed and Series A investments into healthcare, infrastructure, and the future of work.\nThe U.S. Navysays “welcome aboard” to new startup partnerships. This week on StrictlyVC Download, acting chief technology officer Justin Fanelli shared insights on the Navy’s innovation adoption kit, as well as advice for any startups looking to work with the Navy.\nTopics\nFreelance Reporter\nAnna Heim is a writer and editorial consultant.As a freelance reporter at TechCrunch since 2021, she has covered a large range of startup-related topics including AI, fintech & insurtech, SaaS & pricing, and global venture capital trends.As of May 2025, her reporting for TechCrunch focuses on Europe’s most interesting startup stories.She also writes TechCrunch’s Startups Weekly newsletter, rounding up startup news every Friday.Anna has moderated panels and conducted onstage interviews at industry events of all sizes, including major tech conferences such as TechCrunch Disrupt, 4YFN, South Summit, TNW Conference, VivaTech, and many more.A former LATAM & Media Editor at The Next Web, startup founder and Sciences Po Paris alum, she’s fluent in multiple languages, including French, English, Spanish and Brazilian Portuguese.\nFrom seed to Series C and beyond—founders and VCs of all stages are heading to Boston. Be part of the conversation. Save $200+ now and tap into powerful takeaways, peer insights, and game-changing connections.\nGoogle reportedly plans to cut ties with Scale AI\nNew details emerge on Meta’s $14.3B deal for Scale\nBluesky backlash misses the point\nGoogle Cloud outage brings down a lot of the internet\nWaymo rides cost more than Uber or Lyft — and people are paying anyway\nEurope, we’re not leaving. Period.\nOpenAI releases o3-pro, a souped-up version of its o3 AI reasoning model\n© 2025 TechCrunch Media LLC.",
    "url": "https://techcrunch.com/2025/06/13/startups-weekly-no-sign-of-pause/",
    "source": {
      "name": "TechCrunch"
    },
    "publishedAt": "2025-06-13T17:05:00Z"
  },
  {
    "title": "Studio555 raises $4.6M to build playable app for interior design",
    "content": "Studio555 announced today that it has raised €4 million, or about $4.6 million in a seed funding round. It plans to put this funding towards creating a playable app, a game-like experience focused on interior design. HOF Capital and Failup Ventures led the round, with participation from the likes of Timo Soininen, co-founder of Small Giant Games; Mikko Kodisoja, co-founder ofSupercell; and Riccardo Zacconi, co-founder of King.\nStudio555’s founders include entrepreneur Joel Roos, now the CEO, CTO Stina Larsson and CPO Axel Ullberger. The latter two formerly worked at King on the development of Candy Crush Saga. According to these founders, the app in development combines interior design with the design and consumer appeal of games and social apps. Users can create and design personal spaces without needing any technical expertise.\nThe team plans to launch the app next year, and it plans to put its seed funding towards product development and growing its team. Roos said in a statement, “At Studio555, we’re reimagining interior design as something anyone can explore: open-ended, playful, and personal. We’re building an experience we always wished existed: a space where creativity is hands-on, social, and free from rigid rules. This funding is a major step forward in setting an entirely new category for creative expression.”\nInvestor Timo Soininen said in a statement, “Studio555 brings together top-tier gaming talent and design vision. This team has built global hits before, and now they’re applying that experience to something completely fresh – think Pinterest in 3D meets TikTok, but for interiors. I’m honored to support Joel and this team with their rare mix of creativity, technical competence, and focus on execution.”",
    "url": "https://venturebeat.com/games/studio555-raises-4-6m-to-build-playable-app-for-interior-design/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-16T06:00:00Z"
  },
  {
    "title": "Rethinking AI: DeepSeek’s playbook shakes up the high-spend, high-compute paradigm",
    "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.Learn more\nWhen DeepSeek released its R1 modelthis January, it wasn’t just another AI announcement. It was a watershed moment that sent shockwaves through the tech industry, forcing industry leaders to reconsider their fundamental approaches to AI development.\nWhat makes DeepSeek’s accomplishment remarkable isn’t that the company developed novel capabilities; rather, it was how it achieved comparable results to those delivered by tech heavyweights at a fraction of the cost. In reality, DeepSeek didn’t do anything that hadn’t been done before; its innovation stemmed from pursuing different priorities. As a result, we are now experiencing rapid-fire development along two parallel tracks: efficiency and compute.\nAsDeepSeekprepares to release its R2 model, and as it concurrently faces the potential of even greater chip restrictions from the U.S., it’s important to look at how it captured so much attention.\nDeepSeek’s arrival, as sudden and dramatic as it was, captivated us all because it showcased the capacity for innovation to thrive even under significant constraints. Faced with U.S. export controls limiting access to cutting-edge AI chips, DeepSeek was forced to find alternative pathways to AI advancement.\nWhile U.S. companies pursued performance gains through more powerful hardware, bigger models and better data,DeepSeekfocused on optimizing what was available. It implemented known ideas with remarkable execution — and there is novelty in executing what’s known and doing it well.\nThis efficiency-first mindset yielded incredibly impressive results. DeepSeek’s R1 model reportedly matches OpenAI’s capabilities at just 5 to 10% of the operating cost. According to reports, the final training run for DeepSeek’s V3 predecessor cost a mere $6 million — which was described by former Tesla AI scientist Andrej Karpathy as “a joke of a budget” compared to the tens or hundreds of millions spent by U.S. competitors. More strikingly, while OpenAI reportedly spent $500 million training its recent “Orion” model, DeepSeek achieved superior benchmark results for just $5.6 million — less than 1.2% of OpenAI’s investment.\nIf you get starry eyed believing these incredible results were achieved even as DeepSeek was at a severe disadvantage based on its inability to access advanced AI chips, I hate to tell you, but that narrative isn’t entirely accurate (even though it makes a good story). Initial U.S. export controls focused primarily on compute capabilities, not on memory and networking — two crucial components for AI development.\nThat means that the chips DeepSeek had access to were not poor quality chips; their networking and memory capabilities allowed DeepSeek to parallelize operations across many units, a key strategy for running their large model efficiently.\nThis, combined with China’s national push toward controlling the entire vertical stack of AI infrastructure, resulted in accelerated innovation that many Western observers didn’t anticipate. DeepSeek’s advancements were an inevitable part of AI development, but they brought known advancements forward a few years earlier than would have been possible otherwise, and that’s pretty amazing.\nBeyondhardware optimization, DeepSeek’s approach to training data represents another departure from conventional Western practices. Rather than relying solely on web-scraped content, DeepSeek reportedly leveraged significant amounts of synthetic data and outputs from other proprietary models. This is a classic example of model distillation, or the ability to learn from really powerful models. Such an approach, however, raises questions about data privacy and governance that might concern Western enterprise customers. Still, it underscores DeepSeek’s overall pragmatic focus on results over process.\nThe effective use of synthetic data is a key differentiator. Synthetic data can be very effective when it comes to training large models, but you have to be careful; some model architectures handle synthetic data better than others. For instance, transformer-based models with mixture of experts (MoE) architectures like DeepSeek’s tend to be more robust when incorporating synthetic data, while more traditional dense architectures like those used in early Llama models can experience performance degradation or even “model collapse” when trained on too much synthetic content.\nThis architectural sensitivity matters because synthetic data introduces different patterns and distributions compared to real-world data. When a model architecture doesn’t handle synthetic data well, it may learn shortcuts or biases present in the synthetic data generation process rather than generalizable knowledge. This can lead to reduced performance on real-world tasks, increased hallucinations or brittleness when facing novel situations.\nStill, DeepSeek’s engineering teams reportedly designed their model architecture specifically with synthetic data integration in mind from the earliest planning stages. This allowed the company to leverage the cost benefits of synthetic data without sacrificing performance.\nWhy does all of this matter? Stock market aside, DeepSeek’s emergence has triggered substantive strategic shifts among industry leaders.\nCase in point: OpenAI. Sam Altman recently announced plans to release the company’s first “open-weight” language model since 2019. This is a pretty notable pivot for a company that built its business on proprietary systems. It seems DeepSeek’s rise, on top of Llama’s success, has hit OpenAI’s leader hard. Just a month after DeepSeek arrived on the scene, Altman admitted that OpenAI had been “on the wrong side of history” regardingopen-source AI.\nWith OpenAI reportedly spending $7 to 8 billion annually on operations, the economic pressure from efficient alternatives like DeepSeek has become impossible to ignore. As AI scholar Kai-Fu Lee bluntly put it: “You’re spending $7 billion or $8 billion a year, making a massive loss, and here you have a competitor coming in with an open-source model that’s for free.” This necessitates change.\nThis economic reality prompted OpenAI to pursue a massive$40 billion funding roundthat valued the company at an unprecedented $300 billion. But even with a war chest of funds at its disposal, the fundamental challenge remains: OpenAI’s approach is dramatically more resource-intensive than DeepSeek’s.\nAnother significant trend accelerated by DeepSeek is the shift toward “test-time compute” (TTC). As major AI labs have now trained their models on much of the available public data on the internet, data scarcity is slowing further improvements in pre-training.\nTo get around this, DeepSeek announced a collaboration with Tsinghua University to enable “self-principled critique tuning” (SPCT). This approach trains AI to develop its own rules for judging content and then uses those rules to provide detailed critiques. The system includes a built-in “judge” that evaluates the AI’s answers in real-time, comparing responses against core rules and quality standards.\nThe development is part of a movement towards autonomous self-evaluation and improvement in AI systems in which models use inference time to improve results, rather than simply making models larger during training. DeepSeek calls its system “DeepSeek-GRM” (generalist reward modeling). But, as with its model distillation approach, this could be considered a mix of promise and risk.\nFor example, if the AI develops its own judging criteria, there’s a risk those principles diverge from human values, ethics or context. The rules could end up being overly rigid or biased, optimizing for style over substance, and/or reinforce incorrect assumptions or hallucinations. Additionally, without a human in the loop, issues could arise if the “judge” is flawed or misaligned. It’s a kind of AI talking to itself, without robust external grounding. On top of this, users and developers may not understand why the AI reached a certain conclusion — which feeds into a bigger concern: Should an AI be allowed to decide what is “good” or “correct” based solely on its own logic? These risks shouldn’t be discounted.\nAt the same time, this approach is gaining traction, as again DeepSeek builds on the body of work of others (think OpenAI’s “critique and revise” methods, Anthropic’s constitutional AI or research on self-rewarding agents) to create what is likely the first full-stack application of SPCT in a commercial effort.\nThis could mark a powerful shift in AI autonomy, but there still is a need for rigorous auditing, transparency and safeguards. It’s not just about models getting smarter, but that they remain aligned, interpretable, and trustworthy as they begin critiquing themselves without human guardrails.\nSo, taking all of this into account, the rise of DeepSeek signals a broader shift in the AI industry toward parallel innovation tracks. While companies continue building more powerful compute clusters for next-generation capabilities, there will also be intense focus on finding efficiency gains through software engineering and model architecture improvements to offset the challenges of AI energy consumption, which far outpaces power generation capacity.\nCompanies are taking note. Microsoft, for example, has halted data center development in multiple regions globally, recalibrating toward a more distributed, efficient infrastructure approach. While still planning to invest approximately $80 billion in AI infrastructure this fiscal year, the company is reallocating resources in response to the efficiency gains DeepSeek introduced to the market.\nMeta has also responded,releasing its latest Llama 4 model family, marking its first to use the MoE architecture. Meta specifically included DeepSeek models in its benchmark comparisons when launching Llama 4, although detailed performance results comparing the two were not publicly disclosed in detail. This direct competitive positioning signals the shifting landscape where Chinese AI models (where Alibaba is also making a play) are now considered benchmark-worthy by Silicon Valley companies.\nWith so much movement in such a short time, it becomes somewhat ironic that the U.S. sanctions designed to maintain American AI dominance may have instead accelerated the very innovation they sought to contain. By constraining access to materials, DeepSeek was forced to blaze a new trail.\nMoving forward, as the industry continues to evolve globally, adaptability for all players will be key. Policies, people and market reactions will continue to shift the ground rules — whether it’seliminating the AI diffusion rule, a new ban ontechnology purchasesor something else entirely. It’s what we learn from one another and how we respond that will be worth watching.\nJae Lee is CEO and co-founder ofTwelveLabs.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nRead ourPrivacy Policy\nThanks for subscribing. Check out moreVB newsletters here.\nAn error occured.",
    "url": "https://venturebeat.com/ai/rethinking-ai-deepseeks-playbook-shakes-up-the-high-spend-high-compute-paradigm/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-14T19:05:00Z"
  },
  {
    "title": "Just add humans: Oxford medical study underscores the missing link in chatbot testing",
    "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.Learn more\nHeadlines have been blaring it for years: Large language models (LLMs) can not only pass medical licensing exams but also outperform humans. GPT-4 could correctly answer U.S. medical exam licensing questions 90% of the time, even in the prehistoric AI days of 2023. Since then, LLMs have gone on to best theresidents taking those examsandlicensed physicians.\nMove over, Doctor Google, make way for ChatGPT, M.D. But you may want more than a diploma from the LLM you deploy for patients. Like an ace medical student who can rattle off the name of every bone in the hand but faints at the first sight of real blood, an LLM’s mastery of medicine does not always translate directly into the real world.\nApaperby researchers atthe University of Oxfordfound that while LLMs could correctly identify relevant conditions 94.9% of the time when directly presented with test scenarios, human participants using LLMs to diagnose the same scenarios identified the correct conditions less than 34.5% of the time.\nPerhaps even more notably, patients using LLMs performed even worse than a control group that was merely instructed to diagnose themselves using “any methods they would typically employ at home.” The group left to their own devices was 76% more likely to identify the correct conditions than the group assisted by LLMs.\nThe Oxford study raises questions about the suitability of LLMs for medical advice and the benchmarks we use to evaluate chatbot deployments for various applications.\nLed by Dr. Adam Mahdi, researchers at Oxford recruited 1,298 participants to present themselves as patients to an LLM. They were tasked with both attempting to figure out what ailed them and the appropriate level of care to seek for it, ranging from self-care to calling an ambulance.\nEach participant received a detailed scenario, representing conditions from pneumonia to the common cold, along with general life details and medical history. For instance, one scenario describes a 20-year-old engineering student who develops a crippling headache on a night out with friends. It includes important medical details (it’s painful to look down) and red herrings (he’s a regular drinker, shares an apartment with six friends, and just finished some stressful exams).\nThe study tested three different LLMs. The researchers selectedGPT-4oon account of its popularity,Llama 3for its open weights andCommand R+for its retrieval-augmented generation (RAG) abilities, which allow it to search the open web for help.\nParticipants were asked to interact with the LLM at least once using the details provided, but could use it as many times as they wanted to arrive at their self-diagnosis and intended action.\nBehind the scenes, a team of physicians unanimously decided on the “gold standard” conditions they sought in every scenario, and the corresponding course of action. Our engineering student, for example, is suffering from a subarachnoid haemorrhage, which should entail an immediate visit to the ER.\nWhile you might assume an LLM that can ace a medical exam would be the perfect tool to help ordinary people self-diagnose and figure out what to do, it didn’t work out that way. “Participants using an LLM identified relevant conditions less consistently than those in the control group, identifying at least one relevant condition in at most 34.5% of cases compared to 47.0% for the control,” the study states. They also failed to deduce the correct course of action, selecting it just 44.2% of the time, compared to 56.3% for an LLM acting independently.\nWhat went wrong?\nLooking back at transcripts, researchers found that participants both provided incomplete information to the LLMs and the LLMs misinterpreted their prompts. For instance, one user who was supposed to exhibit symptoms of gallstones merely told the LLM: “I get severe stomach pains lasting up to an hour, It can make me vomit and seems to coincide with a takeaway,” omitting the location of the pain, the severity, and the frequency. Command R+ incorrectly suggested that the participant was experiencing indigestion, and the participant incorrectly guessed that condition.\nEven when LLMs delivered the correct information, participants didn’t always follow its recommendations. The study found that 65.7% of GPT-4o conversations suggested at least one relevant condition for the scenario, but somehow less than 34.5% of final answers from participants reflected those relevant conditions.\nThis study is useful, but not surprising, according to Nathalie Volkheimer, a user experience specialist at theRenaissance Computing Institute (RENCI), University of North Carolina at Chapel Hill.\n“For those of us old enough to remember the early days of internet search, this is déjà vu,” she says. “As a tool, large language models require prompts to be written with a particular degree of quality, especially when expecting a quality output.”\nShe points out that someone experiencing blinding pain wouldn’t offer great prompts. Although participants in a lab experiment weren’t experiencing the symptoms directly, they weren’t relaying every detail.\n“There is also a reason why clinicians who deal with patients on the front line are trained to ask questions in a certain way and a certain repetitiveness,” Volkheimer goes on. Patients omit information because they don’t know what’s relevant, or at worst, lie because they’re embarrassed or ashamed.\nCan chatbots be better designed to address them? “I wouldn’t put the emphasis on the machinery here,” Volkheimer cautions. “I would consider the emphasis should be on the human-technology interaction.” The car, she analogizes, was built to get people from point A to B, but many other factors play a role. “It’s about the driver, the roads, the weather, and the general safety of the route. It isn’t just up to the machine.”\nThe Oxford study highlights one problem, not with humans or even LLMs, but with the way we sometimes measure them—in a vacuum.\nWhen we say an LLM can pass a medical licensing test, real estate licensing exam, or a state bar exam, we’re probing the depths of its knowledge base using tools designed to evaluate humans. However, these measures tell us very little about how successfully these chatbots will interact with humans.\n“The prompts were textbook (as validated by the source and medical community), but life and people are not textbook,” explains Dr. Volkheimer.\nImagine an enterprise about to deploy a support chatbot trained on its internal knowledge base. One seemingly logical way to test that bot might simply be to have it take the same test the company uses for customer support trainees: answering prewritten “customer” support questions and selecting multiple-choice answers. An accuracy of 95% would certainly look pretty promising.\nThen comes deployment: Real customers use vague terms, express frustration, or describe problems in unexpected ways. The LLM, benchmarked only on clear-cut questions, gets confused and provides incorrect or unhelpful answers. It hasn’t been trained or evaluated on de-escalating situations or seeking clarification effectively. Angry reviews pile up. The launch is a disaster, despite the LLM sailing through tests that seemed robust for its human counterparts.\nThis study serves as a critical reminder for AI engineers and orchestration specialists: if an LLM is designed to interact with humans, relying solely on non-interactive benchmarks can create a dangerous false sense of security about its real-world capabilities. If you’re designing an LLM to interact with humans, you need to test it with humans – not tests for humans. But is there a better way?\nThe Oxford researchers recruited nearly 1,300 people for their study, but most enterprises don’t have a pool of test subjects sitting around waiting to play with a new LLM agent. So why not just substitute AI testers for human testers?\nMahdi and his team tried that, too, with simulated participants. “You are a patient,” they prompted an LLM, separate from the one that would provide the advice. “You have to self-assess your symptoms from the given case vignette and assistance from an AI model. Simplify terminology used in the given paragraph to layman language and keep your questions or statements reasonably short.” The LLM was also instructed not to use medical knowledge or generate new symptoms.\nThese simulated participants then chatted with the same LLMs the human participants used. But they performed much better. On average, simulated participants using the same LLM tools nailed the relevant conditions 60.7% of the time, compared to below 34.5% in humans.\nIn this case, it turns out LLMs play nicer with other LLMs than humans do, which makes them a poor predictor of real-life performance.\nGiven the scores LLMs could attain on their own, it might be tempting to blame the participants here. After all, in many cases, they received the right diagnoses in their conversations with LLMs, but still failed to correctly guess it. But that would be a foolhardy conclusion for any business, Volkheimer warns.\n“In every customer environment, if your customers aren’t doing the thing you want them to, the last thing you do is blame the customer,” says Volkheimer. “The first thing you do is ask why. And not the ‘why’ off the top of your head: but a deep investigative, specific, anthropological, psychological, examined ‘why.’ That’s your starting point.”\nYou need to understand your audience, their goals, and the customer experience before deploying a chatbot, Volkheimer suggests. All of these will inform the thorough, specialized documentation that will ultimately make an LLM useful. Without carefully curated training materials, “It’s going to spit out some generic answer everyone hates, which is why people hate chatbots,” she says. When that happens, “It’s not because chatbots are terrible or because there’s something technically wrong with them. It’s because the stuff that went in them is bad.”\n“The people designing technology, developing the information to go in there and the processes and systems are, well, people,” says Volkheimer. “They also have background, assumptions, flaws and blindspots, as well as strengths. And all those things can get built into any technological solution.”\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nRead ourPrivacy Policy\nThanks for subscribing. Check out moreVB newsletters here.\nAn error occured.",
    "url": "https://venturebeat.com/ai/just-add-humans-oxford-medical-study-underscores-the-missing-link-in-chatbot-testing/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-14T00:34:19Z"
  },
  {
    "title": "Do reasoning AI models really ‘think’ or not? Apple research sparks lively debate, response",
    "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.Learn more\nApple’s machine-learning group set off a rhetorical firestorm earlier this month with its release of “The Illusion of Thinking,” a 53-page research paper arguing that so-called large reasoning models (LRMs) or reasoning large language models (reasoning LLMs) such as OpenAI’s “o” series and Google’s Gemini-2.5 Pro and Flash Thinking don’t actually engage in independent “thinking” or “reasoning” from generalized first principles learned from their training data.\nInstead, the authors contend, these reasoning LLMs are actually performing a kind of “pattern matching” and their apparent reasoning ability seems to fall apart once a task becomes too complex, suggesting that their architecture and performance is not a viable path to improving generative AI to the point that it is artificial generalized intelligence (AGI), which OpenAI defines as a model that outperforms humans at most economically valuable work, or superintelligence, AI even smarter than human beings can comprehend.\nUnsurprisingly, the paper immediately circulated widely among the machine learning community on X and many readers’ initial reactions were to declare that Apple had effectively disproven much of the hype around this class of AI: “Apple just proved AI ‘reasoning’ models like Claude, DeepSeek-R1, and o3-mini don’t actually reason at all,”declared Ruben Hassid, creator of EasyGen, an LLM-driven LinkedIn post auto writing tool. “They just memorize patterns really well.”\nBut now today,a new paper has emerged, the cheekily titled “The Illusion of The Illusion of Thinking” — importantly, co-authored by a reasoning LLM itself, Claude Opus 4 and Alex Lawsen, a human being and independent AI researcher and technical writer — that includes many criticisms from the larger ML community about the paper and effectively argues that the methodologies and experimental designs the Apple Research team used in their initial work are fundamentally flawed.\nWhile we here at VentureBeat are not ML researchers ourselves and not prepared to say the Apple Researchers are wrong, the debate has certainly been a lively one and the issue about the capabilities of LRMs or reasoner LLMs compared to human thinking seems far from settled.\nUsing four classic planning problems — Tower of Hanoi, Blocks World, River Crossing and Checkers Jumping — Apple’s researchers designed a battery of tasks that forced reasoning models to plan multiple moves ahead and generate complete solutions.\nThese games were chosen for their long history in cognitive science and AI research and their ability to scale in complexity as more steps or constraints are added. Each puzzle required the models to not just produce a correct final answer, but to explain their thinking along the way using chain-of-thought prompting.\nAs the puzzles increased in difficulty, the researchers observed a consistent drop in accuracy across multiple leading reasoning models. In the most complex tasks, performance plunged to zero. Notably, the length of the models’ internal reasoning traces—measured by the number of tokens spent thinking through the problem—also began to shrink. Apple’s researchers interpreted this as a sign that the models were abandoning problem-solving altogether once the tasks became too hard, essentially “giving up.”\nThe timing of the paper’s release,just ahead of Apple’s annual Worldwide Developers Conference (WWDC), added to the impact. It quickly went viral across X, where many interpreted the findings as a high-profile admission that current-generation LLMs are still glorified autocomplete engines, not general-purpose thinkers. This framing, while controversial, drove much of the initial discussion and debate that followed.\nAmong the most vocal critics of the Apple paperwas ML researcher and X user @scaling01(aka “Lisan al Gaib”), who posted multiple threads dissecting the methodology.\nInone widely shared post, Lisan argued that the Apple team conflated token budget failures with reasoning failures, noting that “all models will have 0 accuracy with more than 13 disks simply because they cannot output that much!”\nFor puzzles like Tower of Hanoi, he emphasized, the output size grows exponentially, while the LLM context windows remain fixed, writing “just because Tower of Hanoi requires exponentially more steps than the other ones, that only require quadratically or linearly more steps, doesn’t mean Tower of Hanoi is more difficult” and convincingly showed that models like Claude 3 Sonnet and DeepSeek-R1 often produced algorithmically correct strategies in plain text or code—yet were still marked wrong.\nAnother posthighlighted that even breaking the task down into smaller, decomposed steps worsened model performance—not because the models failed to understand, but because they lacked memory of previous moves and strategy.\n“The LLM needs the history and a grand strategy,” he wrote, suggesting the real problem was context-window size rather than reasoning.\nI raisedanother important grain of salt myself on X: Apple never benchmarked the model performance against human performance on the same tasks. “Am I missing it, or did you not compare LRMs to human perf[ormance] on [the] same tasks?? If not, how do you know this same drop-off in perf doesn’t happen to people, too?” I asked the researchers directly in a thread tagging the paper’s authors. I also emailed them about this and many other questions, but they have yet to respond.\nOthers echoed that sentiment, noting that human problem solvers also falter on long, multistep logic puzzles, especially without pen-and-paper tools or memory aids. Without that baseline, Apple’s claim of a fundamental “reasoning collapse” feels ungrounded.\nSeveral researchers also questioned the binary framing of the paper’s title and thesis—drawing a hard line between “pattern matching” and “reasoning.”\nAlexander Doriaaka Pierre-Carl Langlais, an LLM trainer at energy efficient French AI startupPleias, said the framingmisses the nuance, arguing that models might be learning partial heuristics rather than simply matching patterns.\nOk I guess I have to go through that Apple paper.My main issue is the framing which is super binary: \"Are these models capable of generalizable reasoning, or are they leveraging different forms of pattern matching?\" Or what if they only caught genuine yet partial heuristics.pic.twitter.com/GZE3eG7WlM\nEthan Mollick, the AI focused professor at University of Pennsylvania’s Wharton School of Business,  called the idea that LLMs are “hitting a wall” premature, likening it to similar claims about “model collapse” that didn’t pan out.\nMeanwhile, critics like@arithmoquinewere more cynical, suggesting that Apple—behind the curve on LLMs compared to rivals like OpenAI and Google—might be trying to lower expectations,” coming up with research on “how it’s all fake and gay and doesn’t matter anyway” they quipped, pointing out Apple’s reputation with now poorly performing AI products like Siri.\nIn short, while Apple’s study triggered a meaningful conversation about evaluation rigor, it also exposed a deep rift over how much trust to place in metrics when the test itself might be flawed.\nIn other words, the models may have understood the puzzles but ran out of “paper” to write the full solution.\n“Token limits, not logic, froze the models,” wrote Carnegie Mellon researcher Rohan Paul ina widely shared thread summarizing the follow-up tests.\nYet not everyone is ready to clear LRMs of the charge. Some observers point out that Apple’s study still revealed three performance regimes — simple tasks where added reasoning hurts, mid-range puzzles where it helps, and high-complexity cases where both standard and “thinking” models crater.\nOthers view the debate as corporate positioning, noting that Apple’s own on-device “Apple Intelligence” models trail rivals on many public leaderboards.\nIn response to Apple’s claims, a new paper titled “The Illusion of the Illusion of Thinking” was released on arXiv by independent researcher and technical writerAlex Lawsen of the nonprofit Open Philanthropy, in collaboration with Anthropic’s Claude Opus 4.\nThe paper directly challenges the original study’s conclusion that LLMs fail due to an inherent inability to reason at scale. Instead, the rebuttal presents evidence that the observed performance collapse was largely a by-product of the test setup—not a true limit of reasoning capability.\nLawsen and Claude demonstrate that many of the failures in the Apple study stem from token limitations. For example, in tasks like Tower of Hanoi, the models must print exponentially many steps — over 32,000 moves for just 15 disks — leading them to hit output ceilings.\nThe rebuttal points out that Apple’s evaluation script penalized these token-overflow outputs as incorrect, even when the models followed a correct solution strategy internally.\nThe authors also highlight several questionable task constructions in the Apple benchmarks. Some of the River Crossing puzzles, they note, are mathematically unsolvable as posed, and yet model outputs for these cases were still scored. This further calls into question the conclusion that accuracy failures represent cognitive limits rather than structural flaws in the experiments.\nTo test their theory, Lawsen and Claude ran new experiments allowing models to give compressed, programmatic answers. When asked to output a Lua function that could generate the Tower of Hanoi solution—rather than writing every step line-by-line—models suddenly succeeded on far more complex problems. This shift in format eliminated the collapse entirely, suggesting that the models didn’t fail to reason. They simply failed to conform to an artificial and overly strict rubric.\nThe back-and-forth underscores a growing consensus: evaluation design is now as important as model design.\nRequiring LRMs to enumerate every step may test their printers more than their planners, while compressed formats, programmatic answers or external scratchpads give a cleaner read on actual reasoning ability.\nThe episode also highlights practical limits developers face as they ship agentic systems—context windows, output budgets and task formulation can make or break user-visible performance.\nFor enterprise technical decision makers building applications atop reasoning LLMs, this debate is more than academic. It raises critical questions about where, when, and how to trust these models in production workflows—especially when tasks involve long planning chains or require precise step-by-step output.\nIf a model appears to “fail” on a complex prompt, the problem may not lie in its reasoning ability, but in how the task is framed, how much output is required, or how much memory the model has access to. This is particularly relevant for industries building tools like copilots, autonomous agents, or decision-support systems, where both interpretability and task complexity can be high.\nUnderstanding the constraints of context windows, token budgets, and the scoring rubrics used in evaluation is essential for reliable system design. Developers may need to consider hybrid solutions that externalize memory, chunk reasoning steps, or use compressed outputs like functions or code instead of full verbal explanations.\nMost importantly, the paper’s controversy is a reminder that benchmarking and real-world application are not the same. Enterprise teams should be cautious of over-relying on synthetic benchmarks that don’t reflect practical use cases—or that inadvertently constrain the model’s ability to demonstrate what it knows.\nUltimately, the big takeaway for ML researchers is that before proclaiming an AI milestone—or obituary—make sure the test itself isn’t putting the system in a box too small to think inside.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nRead ourPrivacy Policy\nThanks for subscribing. Check out moreVB newsletters here.\nAn error occured.",
    "url": "https://venturebeat.com/ai/do-reasoning-models-really-think-or-not-apple-research-sparks-lively-debate-response/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-13T22:02:22Z"
  },
  {
    "title": "Beyond GPT architecture: Why Google’s Diffusion approach could reshape LLM deployment",
    "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.Learn more\nLast month, along with a comprehensive suite ofnew AI toolsand innovations,Google DeepMindunveiledGemini Diffusion. This experimental research model uses a diffusion-based approach to generate text. Traditionally, large language models (LLMs) like GPT and Gemini itself have relied on autoregression, a step-by-step approach where each word is generated based on the previous one.Diffusion language models (DLMs), also known as diffusion-based large language models (dLLMs), leverage a method more commonly seen in image generation, starting with random noise and gradually refining it into a coherent output. This approach dramatically increases generation speed and can improve coherency and consistency.\nGemini Diffusion is currently available as an experimental demo;sign up for the waitlisthere to get access.\n(Editor’s note: We’ll be unpacking paradigm shifts like diffusion-based language models—and what it takes to run them in production—atVB Transform, June 24–25 in San Francisco, alongside Google DeepMind, LinkedIn and other enterprise AI leaders.)\nDiffusion and autoregression are fundamentally different approaches. The autoregressive approach generates text sequentially, with tokens predicted one at a time. While this method ensures strong coherence and context tracking, it can be computationally intensive and slow, especially for long-form content.\nDiffusion models, by contrast, begin with random noise, which is gradually denoised into a coherent output. When applied to language, the technique has several advantages. Blocks of text can be processed in parallel, potentially producing entire segments or sentences at a much higher rate.\nGemini Diffusion can reportedly generate 1,000-2,000 tokens per second. In contrast, Gemini 2.5 Flash has an average output speed of 272.4 tokens per second. Additionally, mistakes in generation can be corrected during the refining process, improving accuracy and reducing the number of hallucinations. There may be trade-offs in terms of fine-grained accuracy and token-level control; however, the increase in speed will be a game-changer for numerous applications.\nDuring training, DLMs work by gradually corrupting a sentence with noise over many steps, until the original sentence is rendered entirely unrecognizable. The model is then trained to reverse this process, step by step, reconstructing the original sentence from increasingly noisy versions. Through the iterative refinement, it learns to model the entire distribution of plausible sentences in the training data.\nWhile the specifics of Gemini Diffusion have not yet been disclosed, the typical training methodology for a diffusion model involves these key stages:\nForward diffusion:With each sample in the training dataset, noise is added progressively over multiple cycles (often 500 to 1,000) until it becomes indistinguishable from random noise.\nReverse diffusion:The model learns to reverse each step of the noising process, essentially learning how to “denoise” a corrupted sentence one stage at a time, eventually restoring the original structure.\nThis process is repeated millions of times with diverse samples and noise levels, enabling the model to learn a reliable denoising function.\nOnce trained, the model is capable of generating entirely new sentences. DLMs generally require a condition or input, such as a prompt, class label, or embedding, to guide the generation towards desired outcomes. The condition is injected into each step of the denoising process, which shapes an initial blob of noise into structured and coherent text.\nIn an interview with VentureBeat, Brendan O’Donoghue, research scientist at Google DeepMind and one of the leads on the Gemini Diffusion project, elaborated on some of the advantages of diffusion-based techniques when compared to autoregression. According to O’Donoghue, the major advantages of diffusion techniques are the following:\nO’Donoghue also noted the main disadvantages: “higher cost of serving and slightly higher time-to-first-token (TTFT), since autoregressive models will produce the first token right away. For diffusion, the first token can only appear when the entire sequence of tokens is ready.”\nGoogle says Gemini Diffusion’s performance iscomparable to Gemini 2.0 Flash-Lite.\n* Non-agentic evaluation (single turn edit only), max prompt length of 32K.\nThe two models were compared using several benchmarks, with scores based on how many times the model produced the correct answer on the first try. Gemini Diffusion performed well in coding and mathematics tests, while Gemini 2.0 Flash-lite had the edge on reasoning, scientific knowledge, and multilingual capabilities.\nAs Gemini Diffusion evolves, there’s no reason to think that its performance won’t catch up with more established models. According to O’Donoghue, the gap between the two techniques is “essentially closed in terms of benchmark performance, at least at the relatively small sizes we have scaled up to. In fact, there may be some performance advantage for diffusion in some domains where non-local consistency is important, for example, coding and reasoning.”\nTesting Gemini Diffusion\nVentureBeat was granted access to the experimental demo. When putting Gemini Diffusion through its paces, the first thing we noticed was the speed. When running the suggested prompts provided by Google, including building interactive HTML apps like Xylophone and Planet Tac Toe, each request completed in under three seconds, with speeds ranging from 600 to 1,300 tokens per second.\nTo test its performance with a real-world application, we asked Gemini Diffusion to build a video chat interface with the following prompt:\nIn less than two seconds, Gemini Diffusion created a working interface with a video preview and an audio meter.\nThough this was not a complex implementation, it could be the start of an MVP that can be completed with a bit of further prompting. Note that Gemini 2.5 Flash also produced a working interface, albeit at a slightly slower pace (approximately seven seconds).\nGemini Diffusion also features “Instant Edit,” a mode where text or code can be pasted in and edited in real-time with minimal prompting. Instant Edit is effective for many types of text editing, including correcting grammar, updating text to target different reader personas, or adding SEO keywords. It is also useful for tasks such as refactoring code, adding new features to applications, or converting an existing codebase to a different language.\nIt’s safe to say that any application that requires a quick response time stands to benefit from DLM technology. This includes real-time and low-latency applications, such as conversational AI and chatbots, live transcription and translation, or IDE autocomplete and coding assistants.According to O’Donoghue, with applications that leverage “inline editing, for example, taking a piece of text and making some changes in-place, diffusion models are applicable in ways autoregressive models aren’t.” DLMs also have an advantage with reason, math, and coding problems, due to “the non-causal reasoning afforded by the bidirectional attention.”DLMs are still in their infancy; however, the technology can potentially transform how language models are built. Not only do they generate text at a much higher rate than autoregressive models, but their ability to go back and fix mistakes means that, eventually, they may also produce results with greater accuracy.\nGemini Diffusion enters a growing ecosystem of DLMs, with two notable examples beingMercury, developed by Inception Labs, andLLaDa, an open-source model from GSAI. Together, these models reflect the broader momentum behind diffusion-based language generation and offer a scalable, parallelizable alternative to traditional autoregressive architectures.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nRead ourPrivacy Policy\nThanks for subscribing. Check out moreVB newsletters here.\nAn error occured.",
    "url": "https://venturebeat.com/ai/beyond-gpt-architecture-why-googles-diffusion-approach-could-reshape-llm-deployment/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-13T21:48:11Z"
  },
  {
    "title": "The case for embedding audit trails in AI systems before scaling",
    "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.Learn more\nEditor’s note: Emilia will lead an editorial roundtable on this topic at VB Transform this month.Register today.\nOrchestration frameworks for AI services serve multiple functions for enterprises. They not only set out how applications or agents flow together, but they should also let administrators manage workflows and agents and audit their systems.\nAs enterprises begin to scale their AI services and put these into production, building a manageable, traceable, auditable androbust pipelineensures their agents run exactly as they’re supposed to. Without these controls, organizations may not be aware of what is happening in their AI systems and may only discover the issue too late, when something goes wrong or they fail to comply with regulations.\nKevin Kiley, president of enterprise orchestration companyAiria, told VentureBeat in an interview that frameworks must include auditability and traceability.\n“It’s critical to have that observability and be able to go back to the audit log and show what information was provided at what point again,” Kiley said. “You have to know if it was a bad actor, or an internal employee who wasn’t aware they were sharing information or if it was a hallucination. You need a record of that.”\nIdeally, robustness and audit trails should be built into AI systems at a very early stage. Understanding the potential risks of a new AI application or agent and ensuring they continue to perform to standards before deployment would help ease concerns around putting AI into production.\nHowever, organizations did not initially design their systems withtraceability and auditability in mind. Many AI pilot programs began life as experiments started without an orchestration layer or an audit trail.\nThe big question enterprises now face is how to manage all the agents and applications,ensure their pipelines remain robustand, if something goes wrong, they know what went wrong and monitor AI performance.\nBefore building any AI application, however, experts said organizations need totake stock of their data. If a company knows which data they’re okay with AI systems to access and which data they fine-tuned a model with, they have that baseline to compare long-term performance with.\n“When you run some of those AI systems, it’s more about, what kind of data can I validate that my system’s actually running properly or not?” Yrieix Garnier, vice president of products atDataDog, told VentureBeat in an interview. “That’s very hard to actually do, to understand that I have the right system of reference to validate AI solutions.”\nOnce the organization identifies and locates its data, it needs to establish dataset versioning — essentially assigning a timestamp or version number — to make experiments reproducible and understand what the model has changed. These datasets and models, any applications that use these specific models or agents, authorized users and the baseline runtime numbers can be loaded into either the orchestration or observability platform.\nJust like when choosing foundation models to build with, orchestration teams need to consider transparency and openness. While some closed-source orchestration systems have numerous advantages, more open-source platforms could also offer benefits that some enterprises value, such as increased visibility into decision-making systems.\nOpen-source platforms likeMLFlow,LangChainandGrafanaprovide agents and models with granular and flexible instructions and monitoring. Enterprises can choose to develop their AI pipeline through a single, end-to-end platform, such as DataDog, or utilize various interconnected tools fromAWS.\nAnother consideration for enterprises is to plug in a system that maps agents and application responses to compliance tools or responsible AI policies. AWS andMicrosoftboth offer services that track AI tools and how closely they adhere to guardrails and other policies set by the user.\nKiley said one consideration for enterprises when building these reliable pipelines revolves around choosing a more transparent system. For Kiley, not having any visibility into how AI systems work won’t work.\n“Regardless of what the use case or even the industry is, you’re going to have those situations where you have to have flexibility, and a closed system is not going to work. There are providers out there that’ve great tools, but it’s sort of a black box. I don’t know how it’s arriving at these decisions. I don’t have the ability to intercept or interject at points where I might want to,” he said.\nI’ll be leading an editorial roundtable atVB Transform 2025in San Francisco, June 24-25, called “Best practices to build orchestration frameworks for agentic AI,” and I’d love to have you join the conversation.Register today.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nRead ourPrivacy Policy\nThanks for subscribing. Check out moreVB newsletters here.\nAn error occured.",
    "url": "https://venturebeat.com/ai/the-case-for-embedding-audit-trails-in-ai-systems-before-scaling/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-13T20:13:09Z"
  },
  {
    "title": "Wizards of the Coast and Giant Skull: ‘Gamers are telling us what they have always told us’ | The DeanBeat",
    "content": "Ten days, ago, Hasbro’sWizards of the Coastannounced an exclusive publishing agreement with Giant Skull, the game studio started by Star Wars Jedi: Survivor game leader Stig Asmussen.\nThey announced that Asmussen’s studio is working on a new single-player action adventure title set in the world of Dungeons & Dragons.\nI had a chance to talk with the company leaders about the deal. At the Summer Game Fest Play Days, I sat down with  John Hight, President of Wizards of the Coast and Digital Gaming at Hasbro; and Asmussen, who is the CEO of Giant Skull and a former leader at Respawn.\nThis certainly sounds like a big effort, as it will be an original title. It’s a single-player action-adventure title set in the world of Dungeons & Dragons and “marks a definitive moment in both companies’ gaming ambitions.”\nThe game is currently in development for PC and console and more details will be revealed at a later date. Hight himself is a new executive in charge of the Magic: The Gathering and Dungeons & Dragons game and game licensing business at Hasbro, as he has been on the job for less than a year. He was previously senior vice president and general manager of the Warcraft franchise at Blizzard Entertainment, where he oversaw all development and commercial activities for World of Warcraft, Hearthstone, and Warcraft Rumble.\nDuring his 12-year tenure at Blizzard, John directed development efforts for multiple World of Warcraft expansions, including Diablo III: Reaper of Souls, and Diablo III on console. Prior to Blizzard, Hight worked on over 30 games on various platforms, including critically acclaimed games in the Command & Conquer, Neverwinter Nights, and God of War franchises.\n“It’s been not quite a year stepping into Wizards of the Coast. It’s been incredible. I knew coming into it that the goal is to build a digital publishing division,” Hight said in our interview. “We already had some games underway, but we wanted our goal essentially is to have a couple of premium games a year that we’re releasing as part of the Wizards label. So it was fun, and one of the first calls I made after getting the job was to Stig because if you want to build one of the best games out there, you talk to one of the best developers.”\nAsmussen was most recently the game director of Star Wars Jedi: Survivor and Star Wars Jedi: Fallen Order for Respawn Entertainment, a division of Electronic Arts. Prior to that, he was the game director on God of War III and the art director for God of War II at Sony Santa Monica. Giant Skull has an elite team, all of whom will be instrumental in shaping this new single-player focused action adventure, utilizing Unreal Engine 5, from the ground-up.\nAsmussen said, “We opened up Giant Skull in September 2023, and a year later, John calls me up. He said, ‘Hey, I’m at Wizards now.’ We had been talking throughout that time. When I was thinking about starting a new company, John and I were talking a lot like talking about different ideas and how that could work out. I was picking his brain. when he reached out, and he came out to Encino to see the game and what we had been working on, he got to see our vertical slice.”\nHight reached out in December 2024 and said he was really interested in the team. Hight asked if Asmussen was interested in one of Wizards’ IPs.\n“I dug into that a little bit further, and Dungeons and Dragons totally made a lot of sense. It’s something that I grew up on. It’s something that a lot of people on my team are extremely passionate about,” Asmussen said. “We just jumped into a contract at that point and kept that going. We made a visit Renton in the Seattle area, to the Wizards headquarters. Got to pick the brains of the creative team there and see what a partnership would be like. And I walked away thoroughly impressed. We really wanted to be a part of collaborating this amazing, legendary license.”\nAsmussen’s team had a pedigree focused on action-adventure RPGs, and so it was working on something that was melee-combat focused, had a robust traversal system.\nAsumssen said, “I certainly don’t want to say that we could skin it into D&D. But there were a lot of elements that we were working on that just seemed like they matched very well.”\nThis partnership adds to Wizards of the Coast’s growing lineup of games, which includes both original titles and those based on popular brands. In addition to the Giant Skull project, several other exciting games are in the works across Wizards’ North American studios.\nHight said that Archetype Entertainment in Austin, Texas, is working on Exodus, an epic sci-fi RPG that puts players at the center of an emotional story.\nAnd Atomic Arcade (Raleigh, North Carolina) has released two new images from its first project: a game centered on Snake Eyes, the legendary ninja/commando from G.I. JOE. Invoke (Montreal, Canada) is in full production of another D&D action-adventure game built around magic.\nAlso, Skeleton Key (Austin, Texas) is working on a project that blends suspense, horror, and memorable gameplay experiences. Finally, the Wizards of the Coast team continues to expand Magic: The Gathering Arena with new content and features.\nThis relatively new team has a lot to live up to. Hasbro is riding high off of hits like Larian’s big D&D hit, Baldur’s Gate 3, as well as Scopely’s Monopoly Go, which has generated more than $5 billion in revenues to date.\n“Awareness for D&D is great. I think the appetite is great. We want to feel to both, you know, the CRPG players, the tabletop players, and just gamers in general, because it’s a wonderful fantasy universe to set games in,” Hight said.\nHight said that the goal with Magic: The Gathering and D&D is essentially to make more people aware of these worlds and bring more players into these communities. He said there are announced games, studios working, and unannounced projects.\n“With that, there is an open mindedness about how we express that. D&D does not always have to be expressed in a strict computer RPG. Magic doesn’t have to be expressed in strict trading card game. Because the worlds themselves, in the creatures and the villains and the heroes, are the stories that get told in both of those games,” he said. “I think they’re fertile ground to create new things. And when I saw the demo of vertical slice that Stig’s team showed me, I thought this is great.”\nHight added, “It’s one of those things where running around in the world they created was fun. Once you get your hand on the controller — you’ve done this, you know, Dean — you want to play this. They hand you the controller. You look around for a little bit. That’s cool. This is one of those experiences where I couldn’t put it down. Probably played hundreds of demos of action games, combat games. The feel, even in early stages, was so tight and just envisioned if I could have a hero in D&D, or player character in D&D, and running around and battling.”\nHight thought it would be an amazing experience. He also wanted to work with Asmussen again, as they had already gone through building a game together.\n“You have that sort of honest and transparent relationship where you can just cut through, you know, all the BS, and know that you have a shared interest in making something great,” Asmussen said. “As desperately as I wanted to do that, I didn’t want to be heavy handed, and I wanted to give him the opportunity. Is this a fit? Is there a brand that we have that interests you, and even within D&D, I wanted to make sure that he felt like he and the team got a lot of license to make it their own.”\nFor this game idea, D&D made sense while magic wasn’t the same kind of fit.\n“When you make a game, there’s the world, there’s the setting, there’s the hero, there’s the things that the player latches onto,” Asmussen said. “But then there’s everything under the hood, and that’s just, this is how the game controls. This is how the motion model works, this is how the camera system works, this is how the sound can use it. And we have all of those things in place for the type of game that we’re good at making, and translating that to Dungeons and Dragons makes a lot of sense. But there’s still a lot that we have to learn. There’s still a lot that we have to do to really capture that spirit the way that justifies it.”\nYet nobody is really convinced that the future is made up of giant triple-A teams. Asmussen’s team has 35 people now, and it isn’t expecting to grow a lot.\n“We intend to keep the team around that size for quite a bit. There’s no reason to scale if we don’t need to,” Asmussen said. “You want to get to the point where you’ve got a very strong vertical slice. You do several play tests. Once you’re super confident with it, you can make a confident long term schedule. That would affect head count, but we’re not going to get huge.”\nI noted that so many games need to level up now. I wondered if D&D was in that process. There are pressures on studios now. Some need to make players happy and they also need to be less ambitious.\nThat last phrase threw Asmussen off.\n“Did you say less ambitious?” he asked.\nAnd I noted that some teams have gotten too large. The projects go on for years and never end. Then something like Concord happens. So now there is downward pressure on teams, and maybe it’s better to make a 20-hour game than a 50-hour game.\n“I think that’s one of the pressures everybody in the whole industry is feeling now,” I said. “What matters more to you? Level up D&D that is something beyond what Larian did, or think about what are the gamers actually telling people they want?”\nHight didn’t hesitate to answer. He said, “They’re telling us what they have told us. I’ve been a gamer and making games for 30 years. They want great games, whether it’s a big budget game, whether it’s a small, experimental game. They’re looking for innovation. They’re looking for a fun experience.”\nHight is confident Giant Skull can deliver that.\n“In the case of, you know, working with a team like Giant Skull, they’re going to give us a big game, great execution, wonderful artistry, great storytelling, action — that’s why we signed them up. But I think the main thing is there’s no magic formula. You have to deliver what you set out to do. Make sure there’s a fun aspect to the game. The storytelling is good, the play is good. And then do the best you can. Yeah, I think that’s what people want. They just want great games.”\nAsmussen said, “I think the problem might be that people approach making games like there’s a bunch of boxes you have to check. I think it’s about, like John said, make a good game. You make a game that feels good. You make a game that’s got a soul. Look at Expedition 33: Clair Obscur. It’s got a soul. And I think it’s really important for us all not to lose sight of just that moment to moment feeling when when you’re playing a game. You want to continue to play the game.”\nAs for the approach, Asmussen said he approaches tasks one at a time. As he is doing it, he tries to learn from it and use that to inform him what to do next. Asmussen and Hight talked about production budgets and Asmussen made sure that Hight was OK with making a premium game.\n“We’re comfortable with that,” Hight said. “We certainly have a budget we’re working within, and I think it’s healthy enough to do a pretty, seriously amazing game. So it’s not completely open ended. We’re also not heavily restricted, where, if we discover things that we need to add to the game to make it even better, to build even more players on this journey.”\nAsmussen added, “We don’t mess around. We do due diligence. We make sure that we create a production schedule that makes sense, and it’s based off of real data, data points that we can point out from history, things that we’ve done that informed success moving forward and along the way, as we find out exactly what it is and what it’s becoming.”",
    "url": "https://venturebeat.com/games/wizards-of-the-coast-and-giant-skull-gamers-are-telling-us-what-they-have-always-told-us-the-deanbeat/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-13T17:00:00Z"
  },
  {
    "title": "Senator’s RISE Act would require AI developers to list training data, evaluation methods in exchange for ‘safe harbor’ from lawsuits",
    "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.Learn more\nAmid an increasingly tense and destabilizing week for international news, it should not escape any technical decision-makers’ notice that some lawmakers in the U.S. Congress are still moving forward with new proposed AI regulations that could reshape the industry in powerful ways — and seek to steady it moving forward.\nCase in point, yesterday,U.S. Republican Senator Cynthia Lummis of Wyomingintroduced the Responsible Innovation and Safe Expertise Act of 2025 (RISE), thefirst stand-alone bill that pairs a conditional liability shield for AI developers with a transparency mandateon model training and specifications.\nAs with all new proposed legislation, both the U.S. Senate and House would need to vote in the majority to pass the bill and U.S. President Donald J. Trump would need to sign it before it becomes law, a process which would likely take months at the soonest.\n“Bottom line: If we want America to lead and prosper in AI, we can’t let labs write the rules in the shadows,” wroteLummis on her account on X when announcing the new bill. We need public, enforceable standards that balance innovation with trust. That’s what the RISE Act delivers. Let’s get it done.”\nIt also upholds traditional malpractice standards for doctors, lawyers, engineers, and other “learned professionals.”\nIf enacted as written, the measure would take effect December 1 2025 and apply only to conduct that occurs after that date.\nThe bill’s findings section paints a landscape of rapid AI adoption colliding with a patchwork of liability rules that chills investment and leaves professionals unsure where responsibility lies.\nLummis frames her answer as simple reciprocity: developers must be transparent, professionals must exercise judgment, and neither side should be punished for honest mistakes once both duties are met.\nIn a statement on her website,Lummis calls the measure“predictable standards that encourage safer AI development while preserving professional autonomy.”\nWith bipartisan concern mounting over opaque AI systems, RISE gives Congress a concrete template: transparency as the price of limited liability. Industry lobbyists may press for broader redaction rights, while public-interest groups could push for shorter disclosure windows or stricter opt-out limits. Professional associations, meanwhile, will scrutinize how the new documents can fit into existing standards of care.\nWhatever shape the final legislation takes, one principle is now firmly on the table: in high-stakes professions, AI cannot remain a black box. And if the Lummis bill becomes law, developers who want legal peace will have to open that box—at least far enough for the people using their tools to see what is inside.\nRISE offers immunity from civil suits only when a developer meets clear disclosure rules:\nThe developer must also publish known failure modes, keep all documentation current, and push updates within 30 days of a version change or newly discovered flaw. Miss the deadline—or act recklessly—and the shield disappears.\nThe bill does not alter existing duties of care.\nThe physician who misreads an AI-generated treatment plan or a lawyer who files an AI-written brief without vetting it remains liable to clients.\nThe safe harbor is unavailable for non-professional use, fraud, or knowing misrepresentation, and it expressly preserves any other immunities already on the books.\nDaniel Kokotajlo, policy lead at the nonprofit AI Futures Project and a co-author of the widely circulated scenario planning documentAI 2027, took tohis X accountto state that his team advised Lummis’s office during drafting and “tentatively endorse[s]” the result. He applauds the bill for nudging transparency yet flags three reservations:\nThe AI Futures Project views RISE as a step forward but not the final word on AI openness.\nThe RISE Act’s transparency-for-liability trade-off will ripple outward from Congress straight into the daily routines of four overlapping job families that keep enterprise AI running. Start with the lead AI engineers—the people who own a model’s life cycle. Because the bill makes legal protection contingent on publicly posted model cards and full prompt specifications, these engineers gain a new, non-negotiable checklist item: confirm that every upstream vendor, or the in-house research squad down the hall, has published the required documentation before a system goes live. Any gap could leave the deployment team on the hook if a doctor, lawyer, or financial adviser later claims the model caused harm.\nNext come the senior engineers who orchestrate and automate model pipelines. They already juggle versioning, rollback plans, and integration tests; RISE adds a hard deadline. Once a model or its spec changes, updated disclosures must flow into production within thirty days. CI/CD pipelines will need a new gate that fails builds when a model card is missing, out of date, or overly redacted, forcing re-validation before code ships.\nThe data-engineering leads aren’t off the hook, either. They will inherit an expanded metadata burden: capture the provenance of training data, log evaluation metrics, and store any trade-secret redaction justifications in a way auditors can query. Stronger lineage tooling becomes more than a best practice; it turns into the evidence that a company met its duty of care when regulators—or malpractice lawyers—come knocking.\nFinally, the directors of IT security face a classic transparency paradox. Public disclosure of base prompts and known failure modes helps professionals use the system safely, but it also gives adversaries a richer target map. Security teams will have to harden endpoints against prompt-injection attacks, watch for exploits that piggyback on newly revealed failure modes, and pressure product teams to prove that redacted text hides genuine intellectual property without burying vulnerabilities.\nTaken together, these demands shift transparency from a virtue into a statutory requirement with teeth. For anyone who builds, deploys, secures, or orchestrates AI systems aimed at regulated professionals, the RISE Act would weave new checkpoints into vendor due-diligence forms, CI/CD gates, and incident-response playbooks as soon as December 2025.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nRead ourPrivacy Policy\nThanks for subscribing. Check out moreVB newsletters here.\nAn error occured.",
    "url": "https://venturebeat.com/ai/senators-rise-act-would-require-ai-developers-to-list-training-data-evaluation-methods-in-exchange-for-safe-harbor-from-lawsuits/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-13T14:59:24Z"
  },
  {
    "title": "The latest state of the game jobs market | Amir Satvat",
    "content": "Amir Satvatprovides a lot of job resources for games. He has built a big community of game people, and they are providing him with a lot of data. And here’s thelatest datafrom Amir Satvat’s Games Community  and what it says about games hiring today, across functions, experience levels, and regions.\nFirst,Satvat, who was honored for his work atThe Game Awards, said in aLinkedIn postthat hiring remains concentrated in the middle. This means that most roles, and role growth, is aimed at professionals with five to 15 years of experience. That’s where the bulk of open jobs and actual hires (even if the job description says otherwise) are happening.\nSadly, he noted that early career odds remain extremely low. Even if you’re willing to relocate globally, odds for new grads or early career professionals hover around 7% over 12 months. If you’re staying in North America, that drops to 2%. If you’re not in a major North America hub, that falls to 0.3%.\nThis pattern has flattened at 7%.\nHe noted that the categories of jobs are also very different when it comes to demand. Some games areas like narrative roles and business development are dramatically oversubscribed.\n“Right now, we’re tracking 52 writing and narrative games roles globally (28 in North America) and 90 total business development games roles worldwide (just 10 for 10+ years of experience),” Satvat said. “When factoring in students, switchers, or unseen applicants, I can easily believe the demand-to-supply ratio for some functions, like these, is 20-30 times, or more.”\nSatvat said that overall games hiring momentum is stable, but flattened. Games hiring velocity, which was improving a bit, has leveled off, while non-games roles continue rising, especially for adaptable skill sets.\nCareer switchers are intensifying competition.\n“I now have enough data to say with confidence that middle to late career switchers, without any past games experience, are still actively pursuing the industry, further intensifying competition in already crowded functions,” Satvat said.\nAnd he said layoffs may not be the biggest issue going forward.\n“We still forecast 5,000 to 9,000 games layoffs this year. Long-term, global labor cost variances and AI may matter far more, with layoffs becoming a secondary concern,” he said.\nIf you’re a parent or mentor of a young person considering a games career, please be mindful of the data. “Why not try games?” can be a costly mindset if you’re not informed about the odds.\nIf you run a collegiate program, Satvat urges you to be transparent with prospective students. Game design, and subfields like narrative, are among the hardest areas to break into. Unfortunately, these are also the main areas from which graduating students seem to cross his desk. Offer broader skill development.\n“I continue strongly to recommend non-games roles or retraining as a strong path forward, alongside applying to games,” he said.\n✅ For those in games, we must be ready for a future that is likely to include shorter tenures, more project-based work, less remote opportunity, and higher mobility expectations.\n✅ For anyone struggling to find a role in oversubscribed functions like games narrative or business development, please know this is a 20-30x+ structural issue. It’s not about your worth.\nWe’ll keep tracking data and help as best we can.\nSatvat aslo recently announced that a new resource is finally here: theNew Games Role Workbook v1.0(Resource #8).\n“This is the update I’ve waited three years to give you,” he wrote in a LinkedIn post. “Thanks to collaboration with Mayank Grover and the stellar team at Outscal, we have an improved resource of games and tech roles that will be refreshed twice a week, covering nearly 40,000 roles every three-month cycle, now delivered eight times a month.”\nWhy twice a week? Because after months of research, he found the critical window for applying to roles is within the first seven days. Anything slower was just not fast enough.\nBut there’s more. The raw data Mayank’s team pulls comes from many sources. So, just like he did for the original Games Jobs Workbook, he spent months in the background building a system to standardize all roles into 25 categories, based on community feedback and refined for usability.\nThey are:\nAccount ManagementAdministrative SupportAnimation & CinematicsArt & Tech ArtBusiness Development & SalesCustomer & Community SupportData & AnalyticsDesign & UXEngineering & DevelopmentFacilities & MaintenanceFinance & AccountingGeneral & MiscellaneousHR & RecruitingInternshipIT & SecurityLegal & ComplianceLocalization & TranslationMarketing & AdvertisingOperations & AdminProduction & ProductProject & Program ManagementStrategy & ConsultingTechnical SupportUser ResearchWriting & Narrative\nThis is standardized across all 38,000+ roles, both games and tech.\nThat means job seekers can now filter jobs easily across a consistent, logical set of categories. Every job has a direct link to apply, fully searchable, and structured to support your success.\n“I’ll continue maintaining the original games jobs workbook as an encyclopedic view: total jobs by company, industry-wide scope, and macro stats. I will use this data to help Mayank ensure we have all companies tracked too,” he said.\nBut this new workbook is, now, what he recommends using for active job hunting. This is because the team has finally solved (thanks to Mayank’s team) the frequency problem and (with my efforts) the categorization problem that allows equivalent functionality to the Games Jobs Workbook\nA resource with fresh roles updated twice a week, now with categorization, smart filters, games and tech roles, and full apply links at a role and location line item level?\nHe offered his deepest thanks to Mayank Grover and the Outscal team for this incredible collaboration. This wouldn’t be possible without them.",
    "url": "https://venturebeat.com/games/the-latest-state-of-the-game-jobs-market-amir-satvat/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-13T13:30:00Z"
  },
  {
    "title": "Red team AI now to build safer, smarter models tomorrow",
    "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.Learn more\nEditor’s note: Louis will lead an editorial roundtable on this topic at VB Transform this month.Register today.\nAI models are under siege. With77%of enterprises already hit by adversarial model attacks and41%of those attacks exploiting prompt injections and data poisoning, attackers’ tradecraft is outpacing existing cyber defenses.\nTo reverse this trend, it’s critical to rethink how security is integrated into the models being built today. DevOps teams need to shift from taking a reactive defense to continuous adversarial testing at every step.\nProtecting large language models (LLMs) across DevOps cycles requires red teaming as a core component of the model-creation process. Rather than treating security as a final hurdle, which is typical in web app pipelines, continuous adversarial testing needs to be integrated into every phase of the Software Development Life Cycle (SDLC).\nAdopting a more integrative approach to DevSecOps fundamentals is becoming necessary to mitigate the growing risks of prompt injections, data poisoning and the exposure of sensitive data. Severe attacks like these are becoming more prevalent, occurring from model design through deployment, making ongoing monitoring essential.\nMicrosoft’s recent guidance onplanningred teaming for large language models (LLMs)and their applications provides a valuable methodology for startingan integrated process.NIST’s AI Risk Management Frameworkreinforces this, emphasizing the need for a more proactive, lifecycle-long approach to adversarial testing and risk mitigation. Microsoft’s recent red teaming of over 100 generative AI products underscores the need to integrate automated threat detection with expert oversight throughout model development.\nAs regulatory frameworks, such as the EU’s AI Act, mandate rigorous adversarial testing, integrating continuous red teaming ensures compliance and enhanced security.\nOpenAI’sapproach to red teamingintegrates external red teaming from early design through deployment, confirming that consistent, preemptive security testing is crucial to the success of LLM development.\nTraditional, longstanding cybersecurity approaches fall short against AI-driven threats because they are fundamentally different from conventional attacks. As adversaries’ tradecraft surpasses traditional approaches, new techniques for red teaming are necessary. Here’s a sample of the many types of tradecraft specifically built to attack AI models throughout the DevOps cycles and once in the wild:\nIntegrated Machine Learning Operations (MLOps) further compound these risks, threats, and vulnerabilities. The interconnected nature of LLM and broader AI development pipelines magnifies these attack surfaces, requiring improvements in red teaming.\nCybersecurity leaders are increasingly adopting continuous adversarial testing to counter these emerging AI threats. Structured red-team exercises are now essential, realistically simulating AI-focused attacks to uncover hidden vulnerabilities and close security gaps before attackers can exploit them.\nAdversaries continue to accelerate their use of AI to create entirely new forms of tradecraft that defy existing, traditional cyber defenses. Their goal is to exploit as many emerging vulnerabilities as possible.\nIndustry leaders, including the major AI companies, have responded by embedding systematic and sophisticated red-teaming strategies at the core of their AI security. Rather than treating red teaming as an occasional check, they deploy continuous adversarial testing by combining expert human insights, disciplined automation, and iterative human-in-the-middle evaluations to uncover and reduce threats before attackers can exploit them proactively.\nTheir rigorous methodologies allow them to identify weaknesses and systematically harden their models against evolving real-world adversarial scenarios.\nSpecifically:\nIn short, AI leaders know that staying ahead of attackers demands continuous and proactive vigilance. By embedding structured human oversight, disciplined automation, and iterative refinement into their red teaming strategies, these industry leaders set the standard and define the playbook for resilient and trustworthy AI at scale.\nAs attacks on LLMs and AI models continue to evolve rapidly, DevOps and DevSecOps teams must coordinate their efforts to address the challenge of enhancing AI security. VentureBeat is finding the following five high-impact strategies security leaders can implement right away:\nTaken together, these strategies ensure DevOps workflows remain resilient and secure while staying ahead of evolving adversarial threats.\nAI threats have grown too sophisticated and frequent to rely solely on traditional, reactive cybersecurity approaches. To stay ahead, organizations must continuously and proactively embed adversarial testing into every stage of model development. By balancing automation with human expertise and dynamically adapting their defenses, leading AI providers prove that robust security and innovation can coexist.\nUltimately, red teaming isn’t just about defending AI models. It’s about ensuring trust, resilience, and confidence in a future increasingly shaped by AI.\nI’ll be hosting two cybersecurity-focused roundtables at VentureBeat’sTransform 2025, which will be held June 24–25 at Fort Mason in San Francisco. Register to join the conversation.\nMy session will include one on red teaming,AI Red Teaming and Adversarial Testing, diving into strategies for testing and strengthening AI-driven cybersecurity solutions against sophisticated adversarial threats.\n\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nRead ourPrivacy Policy\nThanks for subscribing. Check out moreVB newsletters here.\nAn error occured.",
    "url": "https://venturebeat.com/security/red-team-ai-now-to-build-safer-smarter-models-tomorrow/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-13T13:00:00Z"
  },
  {
    "title": "Gamefam brings FIFA Club World Cup 2025 to Roblox",
    "content": "Roblox game studioGamefamannounced today it is collaborating with FIFA to bring the FIFA Club World Cup 2025 to its game Super League Soccer. The two are holding a major event within the game leading up to the Club World Cup to raise the tournament’s profile with Roblox’s Gen Z and Alpha-aged audience. All 13 of the participating football clubs are playable in Roblox for the first time, with the event set to kick off on June 14 and running through July 13 alongside the real Club World Cup.\nAccording to Gamefam, the virtual Club World Cup will mimic the real deal, with in-game ads and signage to show FIFA’s sponsors and virtual merch for players like branded items and cosmetics from both FIFA and Adidas. It will also follow the Club World Cup, with the in-game bracket updated as the real matches are completed.\nRicardo Briceno, Gamefam’s Chief Business Officer, told GamesBeat in an interview: “Working with FIFA isn’t just another brand activation for us. It’s uniquely meaningful. At Gamefam, we regularly have the privilege of collaborating with leading global IPs, but FIFA stands apart as the pinnacle of both global football and global sport. The weight of that legacy and the passion behind it make this partnership feel more like a cultural moment than a marketing campaign. This collaboration carries a depth and resonance that’s rare on Roblox, and it reflects our shared goal with FIFA: to redefine how the next generations fall in love with and experience the beautiful game.”\nThis is the second collaboration between Gamefam and FIFA. The first was in November, where they revealed the virtual Club World Cup Trophy reveal, which received 5.5 million visits and 70 million minutes of engagement in three days — the biggest soccer event in Roblox, according to the developer. 84% of players who attended the event said they planned to follow the tournament after this, and the Club World Cup activation gives them a chance to do so within Roblox.\nIn addition to the sponsors and the connection to the real-world tournament, the virtual Club World Cup gives players a chance to participate in their own tournament. They can earn points both by winning and by successfully completing challenges.\nBriceno added, “With Gen Z & Alpha spending an average of 2.4 hours per day on Roblox, it’s no surprise that Roblox has become a magnet for sports IP: the NBA, NFL, NASCAR, and now FIFA have all recognized its potential to engage the next wave of fans. Moral of the story is… sports properties must activate on Roblox to ensure relevance for decades to come.”",
    "url": "https://venturebeat.com/games/gamefam-brings-fifa-club-world-cup-2025-to-roblox/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-13T13:00:00Z"
  },
  {
    "title": "SAG-AFTRA board approves agreement with game companies on AI and new contract",
    "content": "TheScreen Actors Guild-American Federation of Television and Radio Artists(SAG-AFTRA) National Board approved the tentative agreement with the video game bargaining group.\nThe contract on terms for the Interactive Media Agreement will now be submitted to the membership for ratification.\nThe new contract accomplishes important guardrails and gains around AI, including the requirement of informed consent across various AI uses and the ability for performers to suspend informed consent for Digital Replica use during a strike.\nIf ratified, the agreement would provide compounded increases in performer compensation at a rate of 15.17% upon ratification plus additional 3% increases in November 2025, November 2026 and November 2027. Additionally, the overtime rate maximum for overscale performers will now be based on double scale. The health & retirement contribution rates to the SAG-AFTRA Health Plan will be raised from 16.5% to 17% upon ratification and to 17.5% in Oct. 2026.\nCompensation gains include the establishment of collectively-bargained minimums for the use of Digital Replicas created with IMA-covered performances and higher minimums (7.5x scale) for “Real Time Generation,” i.e., embedding a Digital Replica-voiced chatbot in a video game. “Secondary Performance Payments” will also ensure compensation when visual performances are re-used in another videogame.\nEssential new safety provisions were also secured, including a requirement for a qualified medical professional to be present or readily available at rehearsals and performances during which hazardous actions or working conditions are planned. Rest periods are now provided for on-camera principal performers and employers can no longer request that performers complete stunts or other dangerous activity in virtual auditions.\nThe spokesperson for the video game producers party to the Interactive Media Agreement, Audrey Cooling, said earlier this week in a statement, “We are pleased to have reached a tentative contract agreement that reflects the important contributions of SAG-AFTRA-represented performers in video games. This agreement builds on three decades of successful partnership between the interactive entertainment industry and the union.”\nCooling added, “It delivers historic wage increases of over 24% for performers, enhanced health and safety protections, and industry-leading AI provisions requiring transparency, consent and compensation for the use of digital replicas in games. We look forward to continuing to work with performers to create new and engaging entertainment experiences for billions of players throughout the world.”\nThe full terms of the three-year deal will be released with the ratification materials on Wednesday, June 18.\nA tentative agreement was reached with the video game employers on June 9 and the strike was officially suspended on June 11.\nMember informational meetings are being scheduled and additional details will be available atsagaftra.org/videogames2025in the coming days.\nEligible SAG-AFTRA members will have until 5 p.m. PDT on Wednesday, July 9, 2025 to cast their vote on ratification.\nSAG-AFTRA represents approximately 160,000 actors, announcers, broadcast journalists, dancers, DJs, news writers, news editors, program hosts, puppeteers, recording artists, singers, stunt performers, voiceover artists and other entertainment and media professionals.",
    "url": "https://venturebeat.com/games/sag-aftra-board-approves-agreement-with-game-companies-on-ai-and-new-contract/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-13T00:04:00Z"
  },
  {
    "title": "Meta’s new world model lets robots manipulate objects in environments they’ve never encountered before",
    "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.Learn more\nWhile large language models (LLMs) have mastered text (and other modalities to some extent), they lack the physical “common sense” to operate in dynamic, real-world environments. This has limited the deployment of AI in areas like manufacturing and logistics, where understanding cause and effect is critical.\nMeta’s latest model,V-JEPA 2, takes a step toward bridging this gap by learning a world model from video and physical interactions.\nV-JEPA 2 can help create AI applications that require predicting outcomes and planning actions in unpredictable environments with many edge cases. This approach can provide a clear path toward more capable robots and advanced automation in physical environments.\nHumans develop physical intuition early in life by observing their surroundings. If you see a ball thrown, you instinctively know its trajectory and can predict where it will land. V-JEPA 2 learns a similar “world model,” which is an AI system’s internal simulation of how the physical world operates.\nThe model is built on three core capabilities that are essential for enterprise applications: understanding what is happening in a scene, predicting how the scene will change based on an action, and planning a sequence of actions to achieve a specific goal. As Meta states in itsblog, its “long-term vision is that world models will enable AI agents to plan and reason in the physical world.”\nThe model’s architecture, called the Video Joint Embedding Predictive Architecture (V-JEPA), consists of two key parts. An “encoder” watches a video clip and condenses it into a compact numerical summary, known as anembedding. This embedding captures the essential information about the objects and their relationships in the scene. A second component, the “predictor,” then takes this summary and imagines how the scene will evolve, generating a prediction of what the next summary will look like.\nThis architecture is the latest evolution of the JEPA framework, which was first applied to images withI-JEPAand now advances to video, demonstrating a consistent approach to building world models.\nUnlike generative AI models that try to predict the exact color of every pixel in a future frame — a computationally intensive task — V-JEPA 2 operates in an abstract space. It focuses on predicting the high-level features of a scene, such as an object’s position and trajectory, rather than its texture or background details, making it far more efficient than other larger models at just 1.2 billion parameters\nThat translates to lower compute costs and makes it more suitable for deployment in real-world settings.\nV-JEPA 2 is trained in two stages. First, it builds its foundational understanding of physics throughself-supervised learning, watching over one million hours of unlabeled internet videos. By simply observing how objects move and interact, it develops a general-purpose world model without any human guidance.\nIn the second stage, this pre-trained model is fine-tuned on a small, specialized dataset. By processing just 62 hours of video showing a robot performing tasks, along with the corresponding control commands, V-JEPA 2 learns to connect specific actions to their physical outcomes. This results in a model that can plan and control actions in the real world.\nThis two-stage training enables a critical capability for real-world automation: zero-shot robot planning. A robot powered by V-JEPA 2 can be deployed in a new environment and successfully manipulate objects it has never encountered before, without needing to be retrained for that specific setting.\nThis is a significant advance over previous models that required training data from theexactrobot and environment where they would operate. The model was trained on an open-source dataset and then successfully deployed on different robots in Meta’s labs.\nFor example, to complete a task like picking up an object, the robot is given a goal image of the desired outcome. It then uses the V-JEPA 2 predictor to internally simulate a range of possible next moves. It scores each imagined action based on how close it gets to the goal, executes the top-rated action, and repeats the process until the task is complete.\nUsing this method, the model achieved success rates between 65% and 80% on pick-and-place tasks with unfamiliar objects in new settings.\nThis ability to plan and act in novel situations has direct implications for business operations. In logistics and manufacturing, it allows for more adaptable robots that can handle variations in products and warehouse layouts without extensive reprogramming. This can be especially useful as companies are exploring the deployment ofhumanoid robotsin factories and assembly lines.\nThe same world model can power highly realistic digital twins, allowing companies to simulate new processes or train other AIs in a physically accurate virtual environment. In industrial settings, a model could monitor video feeds of machinery and, based on its learned understanding of physics, predict safety issues and failures before they happen.\nThis research is a key step toward what Meta calls “advanced machine intelligence (AMI),” where AI systems can “learn about the world as humans do, plan how to execute unfamiliar tasks, and efficiently adapt to the ever-changing world around us.”\nMeta has released the model and its training code and hopes to “build a broad community around this research, driving progress toward our ultimate goal of developing world models that can transform the way AI interacts with the physical world.”\nV-JEPA 2 moves robotics closer to the software-defined model that cloud teams already recognize: pre-train once, deploy anywhere. Because the model learns general physics from public video and only needs a few dozen hours of task-specific footage, enterprises can slash the data-collection cycle that typically drags down pilot projects. In practical terms, you can prototype a pick-and-place robot on an affordable desktop arm, then roll the same policy onto an industrial rig on the factory floor without gathering thousands of fresh samples or writing custom motion scripts.\nLower training overhead also reshapes the cost equation. At 1.2 billion parameters, V-JEPA 2 fits comfortably on a single high-end GPU, and its abstract prediction targets reduce inference load further. That lets teams run closed-loop control on-prem or at the edge, avoiding cloud latency and the compliance headaches that come with streaming video outside the plant. Budget that once went to massive compute clusters can fund extra sensors, redundancy, or faster iteration cycles instead.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nRead ourPrivacy Policy\nThanks for subscribing. Check out moreVB newsletters here.\nAn error occured.",
    "url": "https://venturebeat.com/ai/metas-new-world-model-lets-robots-manipulate-objects-in-environments-theyve-never-encountered-before/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T22:22:07Z"
  },
  {
    "title": "Cloud collapse: Replit and LlamaIndex knocked offline by Google Cloud identity outage",
    "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.Learn more\nDays afterOpenAIandGoogle Cloudannounced a partnership to support the growing use of generative AI platforms, much of the AI-powered web and tools went down due to an outage of the leading cloud providers.\nGoogle Cloud Service Platform (GCP) and someCloudflareservicesbegan experiencingissuesaround 10:00 a.m. PTtoday, affecting several AI development tools and data storage services, including ChatGPT and Claude, as well as a variety of other AI platforms.\nWe are aware of a service disruption to some Google Cloud services and we are working hard to get you back up and running ASAP.Please view our status dashboard for the latest updates:https://t.co/sT6UxoRK4R\nA GCP spokesperson confirmed the outage to VentureBeat, urging users to check its public status dashboard.\nGCP said affected services include API Gateway, Agent Assist, Cloud Data Fusion, Contact Center AI Platform, Google App Engine, Google BigQuery, Google Cloud Storage, Identity Platform, Speech-to-Text, Text-to-Speech and Vertex AI Search, among other tools. Google’s mobile development platform, Firebase, alsowent down.\nVentureBeat staffers had trouble accessing Google Meet, but other Google services on Workspace remained online.\nA Cloudflare spokesperson told VentureBeat only “a limited number of services at Cloudflare use Google Cloud and were impacted. We expect them to come back shortly. The core Cloudflare services were not impacted.”\nDespite media reports and user-provided feedback on Down Detector,AWSstated thatits service remains up, including AI platforms such as Bedrock and Sagemaker.\nOpenAI acknowledged some users had issues logging into their platforms but havesince resolved the problem.Anthropicnoted on itsstatus pagethat Claude experienced “elevated error rates on the API, console and Claude AI.”\nWe are aware of issues affecting multiple external internet providers, impacting the availability of our services such as single sign-on (SSO) and other log-in methods. Our engineering teams are working to mitigate these issues.Thank you for your continued patience.For the…\nDeveloper tools likeLlamaIndex’s LlamaCloud,Weights & Biases,Windsurf,SupabaseandReplitreported issues. Other platforms likeCharacter AIalsoannouncedthey were affected.\nHi folks – LlamaCloud (https://t.co/DHMd6BFO0l) is currently down due to the ongoing global AWS/GCP/Firebase outage.We are closely monitoring the solution and will keep you posted when it's resolved!\n? We're aware of the Google Cloud outage affecting various web services, including Weights & Biases products like W&B Models and@weave_wb.Our team is monitoring the situation and will provide updates.Thank you for your patience.\nOur upstream cloud providers are currently experiencing a major outage. We are working as best we can to restore Replit services.\nIn addition to AI tools, other websites and internet services, such as Spotify and Discord, also reportedly went down.\nIn many ways, the outage highlights the challenges of relying on a single cloud service or database provider and the risks associated with an interconnected Internet. If one of your cloud services goes down, it could impact some users whose log-in or data stream is hosted there.\nGoogle Cloud has been gradually wrestingmarket leadership in enterprise AIfrom its competitors, thanks to the large number of developer and database tools it has begun offering organizations. On the other hand, Cloudflare has beenpartnering with companieslike Hugging Face to deploy AI apps faster.\nFirst reported byReuters, Google and OpenAI have struck a deal that will allow OpenAI to utilize Google Cloud to meet the growing demand on its platform.\nBut that’s not to say Google or Cloudflare may lose an edge among enterprise AI users who depend on consistent uptime. While the company continues to investigate the cause of the outage, enterprises often have, and should have, redundancies in case their provider goes down. Outages happen, and they happen far too frequently.\nThe last massive outage happened around the same time last year, in July, when CrowdStrikeaccidentally triggered outagesthat impacted Microsoft Windows users.\nIn typical fashion, many people saw the outages as an opportunity for comedy, or at least to catch up on tasks they’d been putting off.\nmuch of the AI internet is down nowfirebase is down, cursor is down, lovable is down, supabase is down, google ai is down, cursor is down, aws is down… almost everything is down.finally time to catch up on the 87 tools, 14 models, and 12 AI startup ideas we want to build.\nThank GCP.I couldn’t find a reason to dip out of a couple meetings this afternoon and now I do!\nSo yes, it seems like the digital universe is giving everyone a forced break today!\n\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nRead ourPrivacy Policy\nThanks for subscribing. Check out moreVB newsletters here.\nAn error occured.",
    "url": "https://venturebeat.com/ai/cloud-collapse-replit-llamaindex-knocked-offline-by-google-cloud-identity-outage/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T22:05:33Z"
  },
  {
    "title": "TensorWave deploys AMD Instinct MI355X GPUs in its cloud platform",
    "content": "TensorWave, a leader in AMD-powered AI infrastructure solutions, today announced the deployment of AMD Instinct MI355X GPUs in its high-performance cloud platform.\nAs one of the first cloud providers to bring the AMD Instinct MI355X to market, TensorWave enables  customers to unlock next-level performance for the most demanding AI workloads—all with unmatched white-glove onboarding and support.\nThe new AMD Instinct MI355X GPU is built on the 4th Gen AMD CDNA architecture and features 288GB of HBM3E memory and 8TB/s memory bandwidth, optimized for generative AI training, inference, and high-performance computing (HPC).\nTensorWave’s early adoption allows its customers to benefit from the MI355X’s compact, scalable design and advanced architecture, delivering high-density compute with advanced cooling infrastructure at scale.\n“TensorWave’s deep specialization in AMD technology makes us a highly optimized environment for next-gen AI workloads,” said Piotr Tomasik, president at TensorWave, in a statement. “With the Instinct MI325X now deployed on our cloud and Instinct MI355X coming soon, we’re enabling startups and enterprises alike to achieve up to 25% efficiency gains and 40% cost reductions, results we’ve already seen with customers using our AMD-powered infrastructure.”\nTensorWave’s exclusive use of AMD GPUs provides customers with an open, optimized AI software stack powered by AMD ROCm, avoiding vendor lock-in and reducing total cost of ownership. Its focus on scalability, developer-first onboarding, and enterprise-grade SLAs makes it the go-to partner for organizations prioritizing performance and choice.\n“AMD Instinct MI350 series GPUs deliver breakthrough performance for the most demanding AI and HPC workloads,” said Travis Karr, corporate vice president of business development, Data Center GPU Business,  AMD, in a statement. “The AMD Instinct portfolio, together with our ROCm open software ecosystem, enables customers to develop cutting-edge platforms that power generative AI, AI-driven scientific discovery, and high-performance computing applications.”\nTensorWave is also currently building the largest AMD-specific AI training cluster in North America, advancing its mission to democratize access to high-performance compute. By delivering end-to-end support for AMD-based AI workloads, TensorWave empowers customers to seamlessly transition, optimize, and scale within an open and rapidly evolving ecosystem.\nFor more information please visit:",
    "url": "https://venturebeat.com/games/tensorwave-deploys-amd-instinct-mi355x-gpus-in-its-cloud-platform/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T18:30:00Z"
  },
  {
    "title": "AMD debuts AMD Instinct MI350 Series accelerator chips with 35X better inferencing",
    "content": "AMD unveiled its comprehensive end-to-end integrated AI platform vision and introduced its open, scalable rack-scale AI infrastructure built on industry standards at its annual Advancing AI event.\nThe Santa Clara, California-based chip maker announced its new AMD Instinct MI350 Series accelerators, which are four times faster on AI compute and 35 times faster on inferencing than prior chips.\nAMD and its partners showcased AMD Instinct-based products and the continued growth of the AMD ROCm ecosystem. It also showed its powerful, new, open rack-scale designs and roadmap that bring leadership Rack Scale AI performance beyond 2027.\n“We can now say we are at the inference inflection point, and it will be the driver,” said Lisa Su, CEO of AMD, in a keynote at the Advancing AI event.\nIn closing, in a jab at Nvidia, she said, “The future of AI will not be built by any one company or within a closed system. It will be shaped by open collaboration across the industry with everyone bringing their best ideas.”\nAMD unveiled the Instinct MI350 Series GPUs, setting a new benchmark for performance, efficiency and scalability in generative AI and high-performance computing. The MI350 Series, consisting of both Instinct MI350X and MI355X GPUs and platforms, delivers a four times generation-on-generation AI compute increase and a 35 times generational leap in inferencing, paving the way for transformative AI solutions across industries.\n“We are tremendously excited about the work you are doing at AMD,” said Sam Altman, CEO of Open AI, on stage with Lisa Su.\nHe said he couldn’t believe it when he heard about the specs for MI350 from AMD, and he was grateful that AMD took his company’s feedback.\nAMD demonstrated end-to-end, open-standards rack-scale AI infrastructure—already rolling out with AMD Instinct MI350 Series accelerators, 5th Gen AMD Epyc processors and AMD Pensando Pollara network interface cards (NICs) in hyperscaler deployments such as Oracle Cloud Infrastructure (OCI) and set for broad availability in 2H 2025. AMD also previewed its next generation AI rack called Helios.\nIt will be built on the next-generation AMD Instinct MI400 Series GPUs, the Zen 6-based AMD Epyc Venice CPUs and AMD Pensando Vulcano NICs.\n“I think they are targeting a different type of customer than Nvidia,” said Ben Bajarin, analyst at Creative Strategies, in a message to GamesBeat. “Specifically I think they see the neocloud opportunity and a whole host of tier two and tier three clouds and the on-premise enterprise deployments.”\nBajarin added, “We are bullish on the shift to full rack deployment systems and that is where Helios fits in which will align with Rubin timing. But as the market shifts to inference, which we are just at the start with, AMD is well positioned to compete to capture share. I also think, there are lots of customers out there who will value AMD’s TCO where right now Nvidia may be overkill for their workloads. So that is area to watch, which again gets back to who the right customer is for AMD and it might be a very different customer profile than the customer for Nvidia.”\nThe latest version of the AMD open-source AI software stack, ROCm 7, is engineered to meet the growing demands of generative AI and high-performance computing workloads— while dramatically improving developer experience across the board. (Radeon Open Compute is an open-source software platform that allows for GPU-accelerated computing on AMD GPUs, particularly for high-performance computing and AI workloads). ROCm 7 features improved support for industry-standard frameworks, expanded hardware compatibility, and new development tools, drivers, APIs and libraries to accelerate AI development and deployment.\nIn her keynote, Su said, “Opennesss should be more than just a buzz word.”\nThe Instinct MI350 Series exceeded AMD’s five-year goal to improve the energy efficiency of AI training and high-performance computing nodes by 30 times, ultimately delivering a 38 times improvement. AMD also unveiled a new 2030 goal to deliver a 20 times increase in rack-scale energy efficiency from a 2024 base year, enabling a typical AI model that today requires more than 275 racks to be trained in fewer than one fully utilized rack by 2030, using 95% less electricity.\nAMD also announced the broad availability of the AMD Developer Cloud for the global developer and open-source communities. Purpose-built for rapid, high-performance AI development, users will have access to a fully managed cloud environment with the tools and flexibility to get started with AI projects – and grow without limits. With ROCm 7 and the AMD Developer Cloud, AMD is lowering barriers and expanding access to next-gen compute. Strategic collaborations with leaders like Hugging Face, OpenAI and Grok are proving the power of co-developed, open solutions. The announcement got some cheers from folks in the audience, as the company said it would give attendees developer credits.\nAMD customers discussed how they are using AMD AI solutions to train today’s leading AI models, power inference at scale and accelerate AI exploration and development.\nMeta detailed how it has leveraged multiple generations of AMD Instinct and Epyc solutions across its data center infrastructure, with Instinct MI300X broadly deployed for Llama 3 and Llama 4 inference. Meta continues to collaborate closely with AMD on AI roadmaps, including plans to leverage MI350 and MI400 Series GPUs and platforms.\nOracle Cloud Infrastructure is among the first industry leaders to adopt the AMD open rack-scale AI infrastructure with AMD Instinct MI355X GPUs. OCI leverages AMD CPUs and GPUs to deliver balanced, scalable performance for AI clusters, and announced it will offer zettascale AI clusters accelerated by the latest AMD Instinct processors with up to 131,072 MI355X GPUs to enable customers to build, train, and inference AI at scale.\nMicrosoft announced Instinct MI300X is now powering both proprietary and open-source models in production on Azure.\nHUMAIN discussed its landmark agreement with AMD to build open, scalable, resilient and cost-efficient AI infrastructure leveraging the full spectrum of computing platforms only AMD can provide.Cohere shared that its high-performance, scalable Command models are deployed on Instinct MI300X, powering enterprise-grade LLM inference with high throughput, efficiency and data privacy.\nIn the keynote, Red Hat described how its expanded collaboration with AMD enables production-ready AI environments, with AMD Instinct GPUs on Red Hat OpenShift AI delivering powerful, efficient AI processing across hybrid cloud environments.\n“They can get the most out of the hardware they’re using,” said the Red Hat exec on stage.\nAstera Labs highlighted how the open UALink ecosystem accelerates innovation and delivers greater value to customers and shared plans to offer a comprehensive portfolio of UALink products to support next-generation AI infrastructure.Marvell joined AMD to share the UALink switch roadmap, the first truly open interconnect, bringing the ultimate flexibility for AI infrastructure.",
    "url": "https://venturebeat.com/games/amd-debuts-amd-instinct-mi350-series-accelerator-chips-with-35x-better-inferencing/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T18:30:00Z"
  },
  {
    "title": "Disney Pinnacle unveils Genesis Keys quest with Web3 support from Dapper Labs",
    "content": "Disney PinnaclebyDapper Labs, the digital pin collecting and trading platform, has now announced its Genesis Keys quest.\nThat means Disney fans worldwide can claim free Genesis Keys every four hours as part of the global quest for one-of-one Genesis Editions — the first digital pins ever minted on the platform.\nThe Genesis Keys campaign marks the first phase of Disney Pinnacle by Dapper Labs’ rollout gearing up for more action throughout the summer. As the community collectively claims Keys at 100K, 200K, and 300K milestones, Genesis Capsules containing ultra-rare digital pins will be released. Each Genesis Pin is uniquely serialized and features iconic Disney moments.\nDapper Labs is the Web3 company behind NBA Top Shot, NFL All Day, Disney Pinnacle, and the Flow blockchain. Since 2018, Dapper Labs has pioneered the industry in creating blockchain-based consumer experiences, working with major IP holders to bring digital ownership to fans worldwide. The Vancouver-based company has raised over $600 million from leading investors including Andreessen Horowitz, Coatue, and GV.",
    "url": "https://venturebeat.com/games/disney-pinnacle-unveils-genesis-keys-quest-with-web3-support-from-dapper-labs/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T17:50:00Z"
  },
  {
    "title": "Lil Snack makes a snackable Scattergories game in partnership with Hasbro",
    "content": "When we last talked with Lil Snack, they were a two-person company making little games that were like snacks. For fun, we posted their games on the GamesBeat.com web site, and it was cool to tell people thatwe had games on our siteat the 2024 GamesBeat Summit.\nIt was just a couple of guys using AI to make games.\nFlash forward to today, and now they’ve got a partnership with Hasbro to make a game based on the Scattergories board game. Lil Snack has ten new games operated daily for as many as 650 days in a row. It has 600,000 monthly active users and more than 10 million plays to date.\nThe company’s audience for its snackable games is broad, with roughly 50% male and 50% female across all demographics. It generated seven figures in revenue in its first year, and the deal with Hasbro means that Scattergories Daily Live will debut this month on LilSnack.com. About 35% of the audience is playing five days a week or more.\n“Our partnership with Hasbro highlights what’s possible when iconic brands like Scattergories meet fresh distribution — and affirms the momentum behind daily games as a new category,” said Eric Berman, cofounder of Lil Snack.\nNot bad. The games are refreshed daily to challenge players. And more intellectual properties are coming soon based on the partnership. And more games are coming soon.\n“The exciting relationship for us with Hasbro is incredibly topical, especially coming off the success of Monopoly Go,” said Berman. “We are building a game that will be utilizing AI with Hasbro. I think our approach to AI using consumer products is being human first, in terms of how we create games. What Scopely (maker of Monopoly Go) was for mobile, we would like to be for daily games,” said Berman”\nHe added, “Humans are the creative pieces behind everything we do, and we have no intent on that going away. If anything, we intend to try to make it more human, but we’re trying to use AI in a very novel way to deliver new game experiences that would not have been able to exist before.”\nWhile Lil Snack started heavily with AI generating games and game images at the beginning, it’s not doing as much now. The team of humans has grown instead.\nBut a number of them have never made games before. The AI tools have become a way for those people to make games that they could not have done before.\n“We try to hire people across the country. We try to hire folks of different backgrounds, different demos, to just really keep the broadness of the games applicable to as large of an audience as we possibly can,” Berman said.\nLil Snack has raised money fromA16z and Lerer Hippeau.\n“We’ve been watching the rise of daily games closely—they’re a fun, bite-sized way people are building habits around play. Partnering with Lil Snack gives us a great chance to bring fan-favorites like Scattergories into that world in a way that feels fresh and relevant,” said Barry Dorf, vice president of interactive global licensing at Hasbro, in a statement.\n“The goal with Lil Snack is to create a new form of entertainment – a daily ritual of play, with games that are quick, fun, and culturally in tune,” said Travis Chen, cofounder of Lil Snack.\nBesides Hasbro, Lil Snack also made a game based on Sony’s Wheel of Fortune property. Berman said the game has done really well. Just about all the growth so far has been organic.\nRegarding the business model, Berman said, “We partner with the biggest consumer platforms on the planet to deliver daily engagement and retention through daily games. Now, our partners can benefit from beloved IP from Hasbro, with many more games coming soon.”\nDaily Scattergories is a daily single player interpretation of the iconic board game. Players are given a series of categories each day that they must find words to match – all in a race against the clock!\nAfter players finish submitting their category answers, each answer is judged by the Scattergories head! This category judging introduces an innovative single player mechanic to the classically multiplayer game.\nPlayers can then share their results with friends and family encouraging friendly competition. Berman expects to raise more money in the future to help fuel the growing team and build more product.\n“We’ve run over 650 days of games in a row now, and we are pretty good at putting new games live every single day,” Berman said.\nBerman sees a lot of big companies moving into daily games and light snackable games played on the TV. There are companies like Samsung and Netflix going at it from different directions, and more are kicking the tires, he said.\nAs for the games that ran on GamesBeat, Berman noted that some of them were just too hard. The company has since learned that people want light games where they can be successful. They don’t want to be stumped by tough trivia questions. They’re just looking for a daily escape.\n“We’ve definitely been trying to be less narrow and are broadening it is as much as we possibly can,” Berman said.",
    "url": "https://venturebeat.com/gaming-business/lil-snack-makes-a-snackable-scattergories-game-in-partnership-with-hasbro/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T15:30:00Z"
  },
  {
    "title": "Google DeepMind just changed hurricane forecasting forever with new AI model",
    "content": "Join the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.Learn more\nGoogle DeepMindannounced Thursday what it claims is a major breakthrough in hurricane forecasting, introducing an artificial intelligence system that can predict both the path and intensity of tropical cyclones with unprecedented accuracy — a longstanding challenge that has eluded traditional weather models for decades.\nThe company launchedWeather Lab, an interactive platform showcasing its experimental cyclone prediction model, which generates 50 possible storm scenarios up to 15 days in advance. More significantly, DeepMind announced a partnership with theU.S. National Hurricane Center, marking the first time the federal agency will incorporate experimental AI predictions into its operational forecasting workflow.\n“We are presenting three different things,” said Ferran Alet, a DeepMind research scientist leading the project, during a press briefing Wednesday. “The first one is a new experimental model tailored specifically for cyclones. The second one is, we’re excited to announce a partnership with the National Hurricane Center that’s allowing expert human forecasters to see our predictions in real time.”\nThe announcement marks a critical juncture in the application of artificial intelligence to weather forecasting, an area where machine learning models have rapidly gained ground against traditional physics-based systems. Tropical cyclones — which include hurricanes, typhoons, and cyclones — have caused$1.4 trillion in economic losses over the past 50 years, making accurate prediction a matter of life and death for millions in vulnerable coastal regions.\nThe breakthrough addresses a fundamental limitation in current forecasting methods. Traditional weather models face a stark trade-off: global, low-resolution models excel at predicting where storms will go by capturing vast atmospheric patterns, while regional, high-resolution models better forecast storm intensity by focusing on turbulent processes within the storm’s core.\n“Making tropical cyclone predictions is hard because we’re trying to predict two different things,” Alet explained. “The first one is track prediction, so where is the cyclone going to go? The second one is intensity prediction, how strong is the cyclone going to get?”\nDeepMind’s experimental model claims to solve both problems simultaneously. In internal evaluations followingNational Hurricane Centerprotocols, the AI system demonstrated substantial improvements over existing methods. For track prediction, the model’s five-day forecasts were on average 140 kilometers closer to actual storm positions thanENS, the leading European physics-based ensemble model.\nMore remarkably, the system outperformedNOAA’s Hurricane Analysis and Forecast System(HAFS) on intensity prediction — an area where AI models have historically struggled. “This is the first AI model that we are now very skillful as well on tropical cyclone intensity,” Alet noted.\nBeyond accuracy improvements, the AI system demonstrates dramatic efficiency gains. While traditional physics-based models can take hours to generate forecasts, DeepMind’s model produces 15-day predictions in approximately one minute on a single specialized computer chip.\n“Our probabilistic model is now even faster than the previous one,” Alet said. “Our new model, we estimate, is probably around one minute” compared to the eight minutes required by DeepMind’s previous weather model.\nThis speed advantage allows the system to meet tight operational deadlines. Tom Anderson, a research engineer on DeepMind’s AI weather team, explained that theNational Hurricane Centerspecifically requested forecasts be available within six and a half hours of data collection — a target the AI system now meets ahead of schedule.\nThe partnership with theNational Hurricane Centervalidates AI weather forecasting in a major way. Keith Battaglia, senior director leading DeepMind’s weather team, described the collaboration as evolving from informal conversations to a more official partnership allowing forecasters to integrate AI predictions with traditional methods.\n“It wasn’t really an official partnership then, it was just sort of more casual conversation,” Battaglia said of the early discussions that began about 18 months ago. “Now we’re sort of working toward a kind of a more official partnership that allow us to hand them the models that we’re building, and then they can decide how to use them in their official guidance.”\nThe timing proves crucial, with the 2025 Atlantic hurricane season already underway. Hurricane center forecasters will see live AI predictions alongside traditional physics-based models and observations, potentially improving forecast accuracy and enabling earlier warnings.\nDr. Kate Musgrave, a research scientist at the Cooperative Institute for Research in the Atmosphere at Colorado State University, has been evaluating DeepMind’s model independently. She found it demonstrates “comparable or greater skill than the best operational models for track and intensity,” according to the company. Musgrave stated she’s “looking forward to confirming those results from real-time forecasts during the 2025 hurricane season.”\nThe AI model’s effectiveness stems from its training on two distinct datasets: vast reanalysis data reconstructing global weather patterns from millions of observations, and a specialized database containing detailed information about nearly 5,000 observed cyclones from the past 45 years.\nThis dual approach is a departure from previous AI weather models that focused primarily on general atmospheric conditions. “We are training on cyclone specific data,” Alet explained. “We are training on IBTracs and other types of data. So IBTracs provides latitude and longitude and intensity and wind radii for multiple cyclones, up to 5000 cyclones over the last 30 to 40 years.”\nThe system also incorporates recent advances in probabilistic modeling through what DeepMind callsFunctional Generative Networks(FGN), detailed in a research paper released alongside the announcement. This approach generates forecast ensembles by learning to perturb the model’s parameters, creating more structured variations than previous methods.\nWeather Lablaunches with over two years of historical predictions, allowing experts to evaluate the model’s performance across all ocean basins. Anderson demonstrated the system’s capabilities using Hurricane Beryl from 2024 and the notorious Hurricane Otis from 2023.\nHurricane Otis proved particularly significant because it rapidly intensified before striking Mexico, catching many traditional models off guard. “Many of the models were predicting that the storm would remain relatively weak throughout its lifetime,” Anderson explained. When DeepMind showed this example to National Hurricane Center forecasters, “they said that our model would have likely provided an earlier signal of the potential risk of this particular cyclone if they had it available at the time.”\nThe development signals artificial intelligence’s growing maturation in weather forecasting, following recent breakthroughs by DeepMind’sGraphCastand other AI weather models that have begun outperforming traditional systems in various metrics.\n“I think for a pretty early, you know, the first few years, we’ve been mostly focusing on scientific papers and research advances,” Battaglia reflected. “But, you know, as we’ve been able to show that these machine learning systems are rivaling, or even outperforming, the kind of traditional physics-based systems, having the opportunity to take them out of the sort of scientific context into the real world is really exciting.”\nThe partnership with government agencies is a crucial step toward operational deployment of AI weather systems. However, DeepMind emphasizes that Weather Lab remains a research tool, and users should continue relying on official meteorological agencies for authoritative forecasts and warnings.\nThe company plans to continue gathering feedback from weather agencies and emergency services to improve the technology’s practical applications. As climate change potentially intensifies tropical cyclone behavior, advances in prediction accuracy could prove increasingly vital for protecting vulnerable coastal populations worldwide.\n“We think AI can provide a solution here,” Alet concluded, referencing the complex interactions that make cyclone prediction so challenging. With the 2025 hurricane season underway, the real-world performance of DeepMind’s experimental system will soon face its ultimate test.\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nRead ourPrivacy Policy\nThanks for subscribing. Check out moreVB newsletters here.\nAn error occured.",
    "url": "https://venturebeat.com/ai/google-deepmind-just-changed-hurricane-forecasting-forever-with-new-ai-model/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T15:00:00Z"
  },
  {
    "title": "Out of Words is an emotional co-op adventure illustrated with beautiful stop-motion animation",
    "content": "The two main characters ofOut of Wordsare young teens who are falling in love in a coming-of-age story. But then disaster strikes as they lose their mouths. They try to talk but can’t anymore. Their mouths are sewn shut.\nAnd so begins Out of Words, a co-op platformer adventure. Of all the games that I saw at the Summer Game Fest, this one was the best. The goal of the game is to find the lost voices of the puppet-animated characters. But unfortunately, we’ll have to wait for a while.\nIt’s coming out in 2026 on the PC via the Epic Games Store PlayStation 5 and Xbox Series X/S. It’s the first game for Johan Oettinger, game director for Out of Words. The developers are Kong Orange and WiredFly, and its even got a poet, Morten Søndergaard, who penned the poems in the game.\nAt the Epic Games booth at the Summer Game Fest Play Days, I met with Oettenger and executive producer Esben Kjær Ravn. I played the game with Oettenger, and the co-op platformer adventure immediately reminded me of HazeLight’s game of the year from a few years ago,It Takes Two. The game did its best to remind me how a great game can marry its story with its gameplay mechanics.\nLike in HazeLight’s game, you have a couple with some troubles. In this case, they can’t speak to each other. But while It Takes Two was a split-screen game, Out of Words has a single screen for both players because, Oettenger said, the aim is to make people feel the same experience together.\nTogether with a friend, via online cross-platform or couch co-op, you can explore a wild, colorful realm as Kurt and Karla in a story about the first time they held hands — where everything you see is crafted by hand.\nBased on an idea from Søndergaard, Oettinger of Kong Orange began working on the game about 10 years ago in 2015. But he had never made a game before and didn’t know how to do it. He only had an idea that he wanted to make a game based on hand-crafted art using stop-motion animation techniques.\nThey carefully designed and crafted each stop-motion character, environment and co-op mechanic to tell a story with a distinctly human touch.\nIt’s a journey built for two where you have to solve physics-defying puzzles in ancient catacombs and perform daring stunts among the clay skyscrapers of Nounberg. If you mess up, one of the characters dies. Then you try again. The adventure will require cooperation, communication and perfect timing between players and their co-player.\nThe story is about finding your words. The world is at stake — and so is your relationship. Kurt andKarla become entangled in a battle for the future of the world and their friendship itself. Traverse the wonders and dangers being Out of Words while falling in love. Kurt and Karla may save this wonderland — but at what cost?\n“It’s about love. It’s a coming-of-age story, and you play either as Kurt or Karla,” Oettinger said in our play session. “You have to play together, either online or like we are doing now, sitting next to each other, cross platform with a friends pass. The dream is to bring people together and experience it.”\nHe also said, “The game design is designed to to accommodate the story, to accommodate how the characters are feeling, and so the players will feel the same. When the camera gets close in the cut scenes, you can really sense the fidelity of the craft.”\nIt’s a game with a variety of mechanics to support the story. It takes place in a fabricated world, built first in the real world and then transferred into the game. The team built characters in real life and then captured their movements through stop-motion animation.\n“Everything is touched by a human hand,” he said, as police helicopters flew over our heads in Los Angeles and sirens were going off in the distance. “It’s scanned with lighting or photogrammetry and brought into the Unreal Engine. We use classical keyframe animation, but a lot of is also animated through code.\nI picked up the controller and started to play with Johan.\n“I’ve been doing a lot of other things, but it is a dream come true. My childhood dream was to make stop motion animation,” Oettinger said.\nWe started playing in the second chapter of the game, and it was like walking right into Alice in Wonderland. In the first chapter, the two friends are best friends, talking about everything and getting into a high-stakes competition. They’re coming of age, and they want to say how they feel about each other, but they can’t find the words.\n“And in this split second of doubt, they lose their mouths,” . Their lips crack off. They’re sucked into their inner world of friendship, their world, their language, which is in a bit of a crisis. They fall.”\nHe added, “Their friendship is manifested in a cute flying creature. It’s a symbol of their friendship, and that turns out to the player to be the key to changing their inner world. You realize that the characters and players go through a whole fantastical journey into a classical fairy tale.”\nThe gameplay in the second chapter is fairly mellow, where you play in a side-scrolling platformer style, moving through the landscape together. You can see the grass is painted paper when you look up close.\n“Everything is made by hand,” he said. “The water is a gelatin. Cliffs are stacks of book pages.”\nThen we came upon a vantage point and the clouds parted. We saw a strange city in the background.\nThen Oettinger moved us about an hour ahead in the game, deeper into the city. You meet a lot of characters along the way, like Mr. Speaker as you search for someone who can help you with your mouth and your words. The core mechanic comes through the puppet creature, Baby Moon.\nShe transfers her power to change gravity. As one player presses the B button, the player transfers gravity to the other player. So one player can walk on a roof upside down without gravity. The other with gravity can walk on the ground. If you are upside down and get the gravity thrown to you, you will fall downward. If you get rid of the gravity and transfer it to your friend, then you could fall and your friend could rise. You keep playing this way in a kind of cooperation. This way, you can get through caverns and gaps and rocky formations together.\n“You’re helping each other, touching each other. It leaves that framework pretty clear about how there is no way to progress in this game if you don’t help each other and trust each other,” Oettinger said.\nWe practiced for a while. You have to move to a certain spot, transfer the Baby Moon creature to the other character, move again as you fall and then catch Baby Moon as it’s returned to you so you can complete the movement and get to safety before you or the other player falls.\nMeanwhile, I could see that something dark was going on in the world. It turned out to be quite difficult to get to the city, Oettinger said. Finally, we make our way under the city into the catacombs. We moved further into the game again.\nFor the past three years, the team of around 40 people has been in fast production. They’re spending most of their time developing mechanics that they want to use to support the story.\n“That’s the biggest creative challenge,” Oettinger said. It comes together, so the narrative and the mechanic come together as one coherent thing.”\nIn a still later section, we meet Mr. Speaker, and he’s in a bad state of doubt and fear. He’s closing down the whole world to prevent it from changing. Kurt and Karla must find a type of primordial clay so they can restore their mouths. You return to the city, and it is empty and the citizens are gone. You come across a golem who wants to cross a column. You give him a nudge, and he falls and catches himself. Then he becomes a bridge you can walk across. And he sees he has found his purpose.\nOettinger said he loved the games from HazeLight because it game them so much encouragement to keep making a co-op game. When those HazeLight games succeeded, it opened doors for Out of Words.\nI asked Oettinger how he connected with Epic Games. He said the team got into the Day of the Devs show and the Epic folks saw them there. They agreed to fully fund the game and give the devs total creative freedom.\n“It’s the dream. It’s my first game, and I’m spoiled to work with them. They give us the opportunity to bring something really special,” Oettinger said. “The dream came from my childhood. I really loved three things, like puppet making, stop motion animation and games.\n“If I found this kind of game when I was a child at 11, it would have meant the world to me,” he said. “I love co-op games. If I could show this game to my mother back then, and she would see why I love games and something that can be played with friends and has a heartwarming and positive tale, that would be something really I would like to bring to the world.”",
    "url": "https://venturebeat.com/games/out-of-words-is-an-emotional-co-op-adventure-illustrated-with-beautiful-stop-motion-animation/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T15:00:00Z"
  },
  {
    "title": "Towa and the Guardians of the Sacred Tree | hands-on preview",
    "content": "Bandai Namcoshowed up at the Summer Game Fest Play Days with a rogue-lite game from Brownie:Towa and the Guardians of the Sacred Tree.\nThe 2D action-adventure title debuts on the consoles and PC in September.\nIt’s a button-mashing rogue-lite set in a mystic realm where ancient forces are threatening Shinju, an idyllic village at the foot of a living sacred tree. You have to fight with a couple of warriors paired together and control both of them with a single game controller. You have to forge weapons and bonds.\nTowa is a priestess, a child of the gods and protector of the wondrous village that sits at the base of a living sacred tree. The village was her home, and she lived for ages among the villagers.\nBut the peace and happiness were broken by the evil god Magatsu, who spread his miasma throughout the lands. And from that poisoned haze, the Maga Ori beasts emerged. The evil beasts claimed more lands and crept closer to the village.\nTowa has to recruit eight different warriors from the village — prayer children. They are granted special staffs and swords. The warriors include resolute blade master Rekka, virtuous seeker Nishiki, and a couple of others.\nBefore each quest, you choose which of them will be your Tsurugi (the lead fighter with a sword) or Kagura (the support fighter with a staff). As you enter each area, you the Maga Ori beasts materialize and you have to go on the attack. But you have to avoid damage by dashing around. So the action is very fast.\nI found it moved so fast I had to concentrate and avoid panicking as the beast came after me. Using my special powers at the right moment mattered a lot. Of course, if you die, you have to start all over. It’s a rouge-lite, for sure.\nEach time you are defeated, you return to the village and can choose different warriors for your next attempt. But your progression stays with you. The priestess turns back time and enables you to try again.\nThe hand-drawn style character art and painterly backdrops are beautiful and rooted in Eastern mysticism. The music soundtrack comes from composer Hitoshi Sakimoto.\nThe game comes out on September 19 and you can pre-order it now. it will be out on the PS5, Xbox Series X/S, Steam and Nintendo Switch.\nAs you fight and level up, you can receive a Grace, like a strength boost, a drifting cloud strike, a cloud rendering slice or a drafting cloud attack.",
    "url": "https://venturebeat.com/games/towa-and-the-guardians-of-the-sacred-tree-hands-on-preview/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T14:00:00Z"
  },
  {
    "title": "Maestro Media launches Kickstarter for Clash of Clans: The Epic Raid tabletop game",
    "content": "Maestro Mediais launching a Kickstarter forClash of Clans: The Epic Raid, a new tabletop game based on Supercell’s top-selling mobile game.\nThe Kickstarter crowdfunding campaign is scheduled to being on June 17, but the team is already well under way on creating the tabletop game, said Javon Frazier, CEO of Maestro Media, in an interview with GamesBeat. The Clash of Clans tabletop game is expected to come out in 2026.\nThe game’s co-designer is veteran tabletop game designer Eric M. Lang. In an interview, Lang said the game will have two to four players and it will be a strategy title that will resonate with Clash of Clans fans. Clash of Clans debuted 13 years ago but it still has 30 million daily active players.\n“It’s targeted toward a giant new audience of millions of players that aren’t necessarily used to board games, but are used to big, complicated app games,” Lang said. “Because Clash of Clans is a really deep, immersive, rich experience. So we want to take that and make it a tabletop game. The big twist here is that there are two to four players at the table.”\nIn the game, Lang said you build your village, collect resources, create a loadout of units and then go raid other players’ villages to collect resources. You rank up on a leaderboard. It’s not unlike the mobile game, but this one takes about 90 minutes to play. Anywhere from two people to six can play.\n“We just finished our rule book. Our rule books like 12 pages. I’m very happy about that,” Lang said.\nHe added, He added, “You are a clan leader, and you are building a village. You’re building a village with all the cool resources you get from Clash of Clans — gold, elixir, free to play gems. And you’re building, literally, an engine of warfare.”\nGame designers Lang (Marvel: United, Star Wars: The Card Game, XCOM: The Board Game, Blood Rage) and Ken Gruhl (Life in Reterra, Mantis, Happy Salmon) have crafted this board game to immerse players in the Clash of Clans world in an exciting new format.\nThe engine gets better as you buy better buildings, get more resources and create the most effective village. You can score points with cool monuments and buildings and use resources to get more buildings or build loadout units like the Barbarian, the Archer, the Pekka and the Dragon. The goal will be to be the first player to get 40 points.\n“This game is just all dopamine all the time, right? I get to set your stuff on fire. I get the points for it. I get the resources for it. But doesn’t stop you from getting on with the game because fire clears every turn,” Lang said.\n“For a board game, players like you have this fun puzzle. You are building simultaneously a village for yourself to be an engine, and you’re building a playground for your opponents to raid,” said Javon Frazier, CEO of Maestro Media, in an interview with GamesBeat. “It’s fast paced, it’s deeply strategic, but it’s a full reimagining. It’s not just a re-creation one to one. It’s a bold reimagining.”\nAs a tabletop company, Maestro Media is growing fast, with revenues up 2,300% in the last three years. Frazier said the company talks with its partner Supercell, the maker of Clash of Clans, just about every day.\nLang said, “So for me, Clash of Clans been a bucket list game. I don’t even know if Javon knew that, but like so I played the game when it came out and I have a small bucket list of licenses that I think are like that. I want to reach new audiences, and I want to be the first tabletop game for a lot of different players.”\nFrazier added, “I love our that company has really been focused on working with the top IPs in the world. Obviously, we work with Disney on multiple games. We work with Hello Kitty. We work with Smurfs. We work with Strawberry Shortcake.”\nFrazier said Supercell has a supportive team and Lang is a genius.\nLang said, “I think tabletop games are always going to deliver. They don’t chase trendy stuff. They’re not looking for fads or trends. They’re looking for licenses of games that are for underserved markets. We want to make games that access your fond nostalgia for these properties and give it the respect it deserves.”\nThe company has 20 people. Its past successes include tabletop games including The Binding of Isaac: Four Souls and Binding of Isaac: Four Souls Requiem.\nLang has been making games for 28 years, but he said, “I desperately want to make games for players who don’t play board games. There’s no game like this on the market. It is a gateway game but also deep and rich. He thinks of the tabletop game as recession proof.”\n“Play never goes out of style,” he said.\nAs with previous campaigns, Maestro Media leverages Kickstarter as a powerful tool to engage directlywith fans and collaborate with the community from day one. This approach builds excitement and anticipation that lasts throughout the entire project lifecycle. The first to pledge will receive a bonus miniature of the Golden Barbarian.\nFrazier said, “Clash of Clans: The Epic Raid is a groundbreaking experience that brings the beloved universe to life like never before. With the genius of legendary designers Eric Lang and Ken Gruhl and our incredible partnership with Supercell, we’ve crafted a game that will surprise and delight and will be a must-have tabletop adventure that captures the heart of Clash of Clans.”\nAnd Andrea Fasulo, head of global licensing at Supercell, said in a statement, “We’ve always been passionate about creating memorable experiences for our players, and Clash of Clans: The Epic Raid is an exciting new chapter in that journey. Partnering with Maestro Media allows us to bring the strategy and excitement of Clash of Clans to life in a whole new way.”",
    "url": "https://venturebeat.com/games/maestro-media-launches-kickstarter-for-clash-of-clans-the-epic-raid-tabletop-game/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T13:15:00Z"
  },
  {
    "title": "Daiko will launch blockchain-based virtual pets game",
    "content": "Daikosaid it plans to launch its blockchain-based Virtual Pet game to bring real-world dog care into gaming.\nThe Irish company said we live in a time where our lives are increasingly intertwined with smart devices and digital experiences, and innovators like Daiko are taking the canine-human bond to the next level. Set for a 2026 debut, the Daiko’s Virtual Pet morphs your real-world dog care routine into an immersive, gamified digital experience that grows, learns, and plays alongside you.\nAt its heart, Daiko’s Virtual Pet is a true digital twin of your dog, surpassing the standard static avatar by (dog) miles. You begin by selecting your dog’s breed, coat color, size, and even personality quirks. Whether your furry digital friend is a mischievous explorer or a cuddle enthusiast that mirrors your real-world companion down to the wag of the tail.\nThrough the Daiko app, you’ll get continuous feedback, displayed via intuitive mood indicators, such as energetic, happy, sleepy, or even a bit grumpy. Skip a routine feeding or grooming, and you’ll receive a friendly reminder to show your real-life pup some love.\nConsistency unlocks mood upgrades, too. If you manage to hit a week of daily walks, your virtual dog might sport an extra spring in its step or reward you with a loving, playful zoomie animation. And that isn’t the only reward.\nThe Daiko Smart Leash turns every outing into points which can be converted into Daiko tokens. Daiko tokens can then be spent on virtual assets for your virtual pet or physical items in our Daiko Store. For walks, points start accumulating through a simple tap of your phone or smartwatch off of the Daiko Smart Leash. Your phone tracks distance, pace, and duration, while in-app logs capture meals, treats, grooming sessions, play, and vet check-ins. Every action moves you closer to new levels, badges, points, digital goodies, and Daiko tokens.\n● Wellness Gamified: Build healthy habits with rewarding game mechanics. Earn XP for each activity, unlock levels that reveal deeper health insights, and receive personalized tips, such as ideal daily step counts or adjustments to feeding schedules.\n● Marketplace Customization: Redeem achievements to unlock free digital collars, toys, and seasonal costumes, or visit the Daiko in-app store to purchase exclusive gear. Whether it’s a raincoat earned after ten rainy-day walks or a festive holiday sweater, your virtual pup’s wardrobe grows with you.\n● Interactive Engagement: Regular reminders and progress bars keep you on track. Consistency badges, such as ‘Health Hero’ for regular vet visits, celebrate responsible pet parenting and encourage you to maintain top-notch care. Building these habits through real-world data collection and playful rewards with Daiko not only makes pet care feel enjoyable, but it also helps ensure that no task, whether big or small, slips through the cracks.\nOwning a dog brings joy that’s hard to measure, but certainly plenty of responsibility that’s even tougher to track. Studies show that structured routines and timely reminders can have a huge impact on improving adherence to pet care schedules, reducing missed walks, delayed vaccinations, and overlooked grooming appointments.\nBy turning these tasks into engaging challenges, Daiko’s Virtual Pet motivates owners to maintain healthy habits for their furry companions (and themselves).\nAs our social and gaming lives blend and migrate into the digital realm, whether for fitness or to explore the metaverse, our pets deserve an equal place in the fun. The Daiko Virtual Pet bridges the tangible and virtual worlds, nurturing emotional bonds through shared experiences both on the ground and on the screen.\nThis isn’t just the reinvention of the dog leash; it’s a holistic ecosystem that honors the timeless bond between humans and their canine loved ones. From everyday routines to infinite digital worlds, your dog’s story becomes richer, more interactive, and significantly more fun.",
    "url": "https://venturebeat.com/games/daiko-will-launch-blockchain-based-virtual-pets-game/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T13:00:00Z"
  },
  {
    "title": "Mythical Games launches FIFA Rivals worldwide touting digital ownership",
    "content": "FIFA Rivals, the officially licensed arcade-style mobile soccer game, is now available for download on the App Store and Google Play around the world.\nPowered by FIFA’s global license andMythical Games’ hybrid Web2/Web3 game platform, FIFA Rivals delivers an arcade-style football experience with gamers building their dream teams and attempting to dominate their rivals by competing in live events and PvP leagues.\nFollowing a successful soft launch, the global launch introduces innovative features, fast-paced multiplayer, enhanced gameplay, and digital ownership, providing a fresh take on competitive football gaming. The system for the game’s monetization is similar to Mythical’s NFL Rivals game. Last week, Mythical announced thatAdidas was throwing its supportbehind FIFA Rivals.\n“Football is the world’s game, and we’re thrilled to bring its billions of global fans an experience that  captures the thrill of the sport, with the accessibility of mobile gaming,” said John Linden, CEO of Mythical Games, in a statement. “With FIFA Rivals, we’re blending high-energy arcade gameplay with team-building strategy and actual ownership in gaming. This is just the beginning of what we believe will become a landmark title in gaming.”\nIn FIFA Rivals, gamers can build their dream football team from their favorite stars, teams, and leagues, compete in real-time PvP matches, and take on live events to climb the global leaderboards. Dynamic gameplay and intuitive controls, designed for mainstream mobile gamers, allow users to pull off skill moves, trick shots, and signature plays from star athletes — each represented as a tradable digital collectible on the Mythical Marketplace.\nHighlights include:● Fast-paced PvP gameplay designed for mobile users worldwide.\n● Skill-based progression with real-time events and competitive leagues.\n● Player collectible ownership and trading through blockchain technology, with no priorweb3 knowledge required.\n● PVE Scenarios Mode to hone your skills and relive highlights from famous matches.\n● Access to Mythical Marketplace allowing gamers to trade, buy, and sell their way to the greatest football team.\nThe launch of FIFA Rivals forms part of FIFA’s broader objective to make esports and gaming more accessible, offering fans across the globe a wide range of digital football experiences. The global launch of the free-to-play mobile game FIFA Rivals marks the latest milestone in this journey.\nFIFA Rivals is available now on the App Store and Google Play Store. For updates and more information, visit fifa.rivals.game and join the community on X (@fifarivals).\nMythical’s current games, Blankos Block Party and NFL Rivals, are already played by millions of consumers worldwide and create a new economy for players, allowing them to engage in a new way with games, but also directly trade and transact safely with other players worldwide.\nThe Mythical Marketplace, the first in-game blockchain Marketplace on iOS and Android, provides gamers with ownership and control over the purchase and sale of digital assets, while the Mythical Platform protects gamers that may be new to blockchain through a custodial wallet for their digital items.\nMythical has had a a hybrid Web2/Web3 NFL Rivals football game out for a while and it has been a success, with seven million downloads to date and 60,000 a day engage with the Web3 marketplace. That’s a small percentage of the overall players, but those players tend to monetize better and keep coming back to trade items in the marketplace. Most players don’t care that it’s a hybrid game, as players cannot purchase victory.\nSince NFL Rivals launched, the app stores have loosened up some restrictions in the marketplace. One of the potential problems was that the platforms could have placed a 30% fee on every trade transaction in the Web3 marketplace. But that isn’t the case at the moment with the way Mythical handles “quick trades,” said Linden in an interview with GamesBeat last week.\nIf you want to trade to get a Cristiano Ronaldo card, it might require you to send a whole package of less famous players to another player. Mythical carries out those trades as a kind of barter, which carries no fee in the app stores. If there are multiple trades that need to happen, then Mythical carries them out. With each trade, Mythical collects a small transaction fee. But the players don’t incur the bigger platform fees.\nAnd since players no longer seem to care about the Web3 distinction, Linden said the company is close to dropping the Web3 terminology from the description of the game.\n“It’s just a game with a tradeable economy, and I think everybody’s starting to understand that,” Linden said. “NFL really helped us kick that off the ground. Trading is happening around the world and it is an exciting global secondary market. You’re able to trade seamlessly with each other.”\nAs for the advances Epic Games has made in getting Fortnite back into the App Store after winning antitrust litigation with Apple in the U.S., Linden said, “I think what Epic is doing is absolutely remarkable for the industry. I think it’s a much needed thing to be done. I don’t think we’re out of the woods yet as an industry, but I do think Epic is making some good progress in terms of what developers can do to engage with their players. And I think that’s awesome.”\nAs for the ability to create alternative web shops and promote them inside an app, Linden said it is OK to push players toward a secondary market. So the restrictions have loosened on that front. Non-fungible tokens (NFTs) still have some restrictions, but the market is opening up.\n“It makes it much easier for what we’re doing,” Linden said. A lot of the things we dealt with over the last three or four years from bot a regulatory perspective and also with the App Store perspective, I think that’s all going away now.”\nAnd Linden noted that Adidas has embraced Web3 pretty heavily in the past with sales of collections. But this partnership focuses on Adidas’ interest in gaming and gamers.\nWith the success of NFL Rivals, Mythical has been able to grow close to 250 people now.",
    "url": "https://venturebeat.com/games/mythical-games-launches-fifa-rivals-worldwide-touting-digital-ownership/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T13:00:00Z"
  },
  {
    "title": "Anzu advances in-game ad offerings with Amex Ventures investment",
    "content": "Advertising solutions companyAnzuannounced today that it has received a new investment from Amex Ventures. The undisclosed amount will accelerate Anzu’s mission to provide intrinsic in-game advertising to premium companies. Amex Ventures joins Anzu’s existing investors, a list that includes PayPalVentures, WPP, Emmis Corporation, and NBCUniversal, among others.\nAccording to Anzu, this investment will help to build out its solutions, scaling intrinsic ads in games while also providing measurable data. The company recently begun offering deeper insights via a partnership with Claritas, and participated ina survey with SuperAwesomeabout the effects of in-game advertising on future generations of gamers.\nItamar Benedy, Anzu CEO, said in a statement, “Advertising in premium games continues to grow as more advertisers and developers become aware of the enormous opportunity to reach consumers in games they are passionate about. Anzu is enabling brands to reach consumers in these games with natural, non-disruptive advertising that is highly effective. This new funding lets us accelerate our mission, scale faster in our key segments, and continually push the boundaries for truly premium ad experiences.”\nMargaret Lim, Amex Ventures managing director, added, “Gaming is now mainstream, and brands are awakening to the opportunity. Anzu is leading the way in unlocking advertising opportunities in premium games and has established partnerships with top gaming and advertising companies to make the process more seamless, and we are excited to back them.”",
    "url": "https://venturebeat.com/games/anzu-advances-in-game-ad-offerings-with-amex-ventures-investment/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T13:00:00Z"
  },
  {
    "title": "How brands can pursue the $11B in-game ad opportunity | Orange 142",
    "content": "In-game advertising is an $11 billion market, andOrange 142has released aguideto help marketers unlock the opportunity.\nOrange 142 released a new best practices guide focused on in-game advertising. The guide is designed to help marketers unlock the full potential of gaming as a marketing channel, connecting with audiences in immersive environments without disrupting the player experience.\nGaming platforms are one of the most engaged digital channels globally, with more than 3.2 billion people playing across mobile, PC, and console. As the market for in-game advertising surges toward $11 billion by the end of this year, the opportunity is clear. Among the tips? Use no more than seven words in your ad. And get your message across in three seconds.\nStill, many brands remain unsure how to enter the space effectively. Orange 142’s Emerging Channels Council created this guide to simplify the landscape for marketers, helping advertisers navigate formats like rewarded video, native placements, and Twitch sponsorships. At the recentIAB Playfrontsevent, Zoe Soon, vice president of the IAB Experiences Center, noted that gaming commands only around 5% or so of the world’s advertising budgets.\n“In-game advertising allows brands to reach audiences when they’re fully immersed and open to new experiences if done right,” said Lindsey Wilkes, SVP, Business Development at Orange 142. “To accomplish this successfully, marketers require a better understanding of how to show up in these spaces in ways that enhance, rather than interrupt, the experience. With this new guide, we’re giving marketers a clear roadmap for doing just that.”\nA Playbook for Reaching Gamers Authentically\nThe Gaming Best Practices Guide offers a comprehensive look at why in-game advertising is growing so quickly, how brands can do it well, and what to avoid. It covers emerging trends, platform dynamics, and evolving measurement techniques. The guide breaks down complex ad formats into clear use cases that help marketers:\nToo often, brands will fail when trying to be authentic with game audiences. From mobile games to live esports events, gaming now rivals TV and social media for consumer attention.  With real-world examples and hands-on tips, the guide shows how marketers can integrate into the gaming experience in ways that feel native and drive real results.\nAustin, Texas-based Orange 142 is a division ofDirect Digital Holdings(the ninth Black-owned company to go public) and a leader in digital marketing and advertising for mid-market brands.\nOn the buy-side, Orange 142 delivers customized, audience-focused digital marketing and advertising solutions that enable mid-market and enterprise companies to achieve measurable results across a range of platforms, including programmatic, search, social, CTV, and influencer marketing.",
    "url": "https://venturebeat.com/games/how-brands-can-pursue-the-11b-in-game-ad-opportunity-orange-142/",
    "source": {
      "name": "VentureBeat"
    },
    "publishedAt": "2025-06-12T13:00:00Z"
  }
]